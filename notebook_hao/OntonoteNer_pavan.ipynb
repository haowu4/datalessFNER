{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, OrderedDict\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import cPickle as pickle\n",
    "from unidecode import unidecode\n",
    "from dfiner.utils import get_default_config\n",
    "from dfiner.kb_bias.kb_bias_annotator import KBBiasTypeAnnotator\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "class Mention(object):\n",
    "    def __init__(self, start, end, label, tokens, doc_id = \"\"):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.label = label\n",
    "        self.tokens = tokens\n",
    "        self.surface = [tokens[i] for i in range(start,end)]\n",
    "        self.ext_surface = [tokens[i] for i in range(max(start-1,0),min(end+1,len(tokens)))]\n",
    "        self.doc_id = doc_id\n",
    "        doc = nlp.tokenizer.tokens_from_list(tokens)\n",
    "        nlp.tagger(doc)\n",
    "        nlp.parser(doc)\n",
    "        self.doc = doc\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        o = [(\"start\",self.start),\n",
    "            (\"end\",self.end),\n",
    "            (\"surface\",self.surface), \n",
    "            (\"label\",self.label),\n",
    "            (\"doc_id\",self.doc_id),\n",
    "            (\"tokens\",self.tokens)]\n",
    "        \n",
    "        return json.dumps(OrderedDict(o), indent=4)\n",
    "        \n",
    "def read_figer(file = \"/home/haowu4/codes/dataless_finer/eval_figer/data/gold_cleaned_figer_test.label\"):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with codecs.open(file, \"r\", \"utf-8\") as input:\n",
    "        for i, line in enumerate(input):\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                # New sentence\n",
    "                if len(tokens) == 0:\n",
    "                    continue\n",
    "                sent = get_sentence(tokens, labels)\n",
    "                for s in sent:\n",
    "                    sentences.append(s)\n",
    "                tokens = []\n",
    "                labels = []\n",
    "                continue\n",
    "            line = line.split(\"\\t\")\n",
    "            word, label = line[0], line[1]\n",
    "            tokens.append(word)\n",
    "            labels.append(label)\n",
    "            \n",
    "    if len(tokens) == 0:\n",
    "        return sentences\n",
    "\n",
    "    sent = get_sentence(tokens, labels)\n",
    "    sentences.append(sent)\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    return sentences\n",
    "\n",
    "    \n",
    "        \n",
    "def get_sentence(tokens, labels):\n",
    "    entities = []\n",
    "    current_labels = \"\"\n",
    "    current_start = 0\n",
    "    in_mention = False\n",
    "    for i, (t, lab_str) in enumerate(zip(tokens, labels)):\n",
    "        if lab_str.startswith(\"B\"):\n",
    "            if in_mention:\n",
    "                entities.append(Mention(current_start, i, current_labels, tokens))\n",
    "                in_mention = False\n",
    "            current_start = i\n",
    "            current_labels = lab_str.split(\"-\")[1]\n",
    "            in_mention = True\n",
    "\n",
    "        if lab_str == \"O\":\n",
    "            if in_mention:\n",
    "                entities.append(Mention(current_start, i, current_labels, tokens))\n",
    "                in_mention = False\n",
    "   \n",
    "    if in_mention:\n",
    "        entities.append(Mention(current_start, len(labels), current_labels, tokens))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def load_ontonotes(file):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with codecs.open(file, \"r\", \"utf-8\") as input:\n",
    "        for i, line in enumerate(input):\n",
    "            if i < 1:\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                # New sentence\n",
    "                if len(tokens) == 0:\n",
    "                    continue\n",
    "                sent = get_sentence(tokens, labels)\n",
    "                for s in sent:\n",
    "                    sentences.append(s)\n",
    "                tokens = []\n",
    "                labels = []\n",
    "                continue\n",
    "            line = line.split(\"\\t\")\n",
    "            word, label = line[5], line[0]\n",
    "            tokens.append(word)\n",
    "            labels.append(label)\n",
    "            \n",
    "    if len(tokens) == 0:\n",
    "        return sentences\n",
    "\n",
    "    sent = get_sentence(tokens, labels)\n",
    "    sentences.append(sent)\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def load_all_data(base_folder):\n",
    "    ret = []\n",
    "    for f in os.listdir(base_folder):\n",
    "        fn = os.path.join(base_folder,f)\n",
    "        ms = load_ontonotes(fn)\n",
    "        for m in ms:\n",
    "            m.doc_id = f\n",
    "            ret.append(m)\n",
    "    return ret\n",
    "\n",
    "class Lexicon(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.curr = 0\n",
    "        self.m = {}\n",
    "        self.counter = defaultdict(int)\n",
    "        self.counter_per_type = defaultdict(lambda:defaultdict(int))\n",
    "    \n",
    "    def see_feature(self, f, t = None):\n",
    "        self.counter[f] += 1\n",
    "        if t:\n",
    "            self.counter_per_type[f][t] += 1\n",
    "        if f not in self.m:\n",
    "            self.m[f] = self.curr\n",
    "            self.curr += 1\n",
    "            \n",
    "    \n",
    "    def prune(self, min_support):\n",
    "        self.curr = 0\n",
    "        self.m = {}\n",
    "        for k in self.counter:\n",
    "            if self.counter[k] > min_support:\n",
    "                self.m[k] = self.curr\n",
    "                self.curr += 1\n",
    "    \n",
    "    def getOrNegOne(self, f):\n",
    "        if f in self.m:\n",
    "            return self.m[f]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def getOneHot(self, f):\n",
    "        ret = np.zeros((self.curr))\n",
    "        if f in self.m:\n",
    "            ret[self.m[f]] = 1.0\n",
    "        return ret\n",
    "    \n",
    "def loadW2V(w2v_file, allowed = None):\n",
    "    ret = {}\n",
    "    err = 0\n",
    "    with codecs.open(w2v_file, \"r\" , 'utf-8') as input:\n",
    "        for line in input:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            try:\n",
    "                w,vec = line.split(\"\\t\")\n",
    "            except ValueError:\n",
    "#                 print(line)\n",
    "                err += 1\n",
    "                continue\n",
    "            if allowed is not None and w in allowed:\n",
    "                vec = [float(v) for v in vec.split(\" \")]\n",
    "                ret[w] = np.array(vec)\n",
    "    print(\"%d line failed\" % err)\n",
    "    return ret\n",
    "\n",
    "def generate_vecs(objs, \n",
    "                  typ_function,\n",
    "                  feature_func,\n",
    "                  dense_real_vec_features = [],\n",
    "                  lex = None,\n",
    "                  type_lex = None\n",
    "                 ):\n",
    "    len_x = len(objs)\n",
    "    \n",
    "    if lex is None:\n",
    "        lex = Lexicon()\n",
    "        for x in objs:\n",
    "            for ff in feature_func:\n",
    "                for k,v in ff(x.doc, x.start, x.end):\n",
    "                    lex.see_feature(k, typ_function(x))\n",
    "        lex.prune(7)\n",
    "\n",
    "    if type_lex is None:\n",
    "        type_lex = Lexicon()\n",
    "        for x in objs:\n",
    "            y = typ_function(x)\n",
    "            type_lex.see_feature(y)\n",
    "\n",
    "    \n",
    "    \n",
    "    row_ids = []\n",
    "    col_ids = []\n",
    "    vs = []\n",
    "    \n",
    "    dense_vecs = []\n",
    "    \n",
    "    ys = []\n",
    "    \n",
    "    for i, x in enumerate(objs):\n",
    "        for ff in feature_func:\n",
    "            for k,v in ff(x.doc, x.start, x.end):\n",
    "                idx = lex.getOrNegOne(k)\n",
    "                if idx > -1:\n",
    "                    row_ids.append(i)\n",
    "                    col_ids.append(idx)\n",
    "                    vs.append(v)\n",
    "                    \n",
    "        if len(dense_real_vec_features) > 0:\n",
    "            ds = []\n",
    "            for dff in dense_real_vec_features:\n",
    "                v  = dff(x)\n",
    "                ds.append(v)\n",
    "#             print(len(ds))\n",
    "            denv = np.hstack((ds))\n",
    "            dense_vecs.append(denv)\n",
    "            \n",
    "        ys.append(type_lex.getOrNegOne(typ_function(x)))\n",
    "        \n",
    "    sp = (len_x, lex.curr)\n",
    "    print(sp)\n",
    "    xs = coo_matrix((vs, (row_ids, col_ids)), shape=sp)\n",
    "    print(\"dense_vecs[0].shape\", dense_vecs[0].shape)\n",
    "    dense_vecs = np.vstack(dense_vecs)\n",
    "    print(\"dense_vecs.shape\", dense_vecs.shape)\n",
    "    if len(dense_real_vec_features) > 0:\n",
    "        print(\"shapes : \", xs.shape, dense_vecs.shape, )\n",
    "        xs = hstack((xs, dense_vecs))\n",
    "    return lex, type_lex, xs.tocsr(), np.asarray(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159298 mention loaded\n",
      "52467 word loaded\n",
      "1 line failed\n",
      "41459 words have w2v\n"
     ]
    }
   ],
   "source": [
    "def load_cache_or_none(filepath):\n",
    "    return None\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb') as f_in:\n",
    "            data = pickle.load(f_in)\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_cache(obj, filepath):\n",
    "    return None\n",
    "    with open(filepath, 'wb') as f_out:\n",
    "        pickle.dump(obj, f_out, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "train_mentions_cache_file = \"./OntonoteNERCache/TrainAndDev.pkl\"\n",
    "train_mentions = load_cache_or_none(train_mentions_cache_file)\n",
    "if train_mentions is None:\n",
    "    train_mentions= load_all_data(\"/home/haowu4/data/ontonotes_ner/ColumnFormat/TrainAndDev/\")\n",
    "    save_cache(train_mentions, train_mentions_cache_file)\n",
    "\n",
    "\n",
    "test_mentions_cache_file = \"./OntonoteNERCache/Test.pkl\"\n",
    "test_mentions = load_cache_or_none(test_mentions_cache_file)\n",
    "if test_mentions is None:\n",
    "    test_mentions = load_all_data(\"/home/haowu4/data/ontonotes_ner/ColumnFormat/Test/\")\n",
    "    save_cache(test_mentions, test_mentions_cache_file)\n",
    "\n",
    "\n",
    "figer_datas = read_figer()\n",
    "\n",
    "\n",
    "VOCABS = set()\n",
    "mention_counter = 0\n",
    "for mention in [train_mentions,test_mentions, figer_datas]:\n",
    "    for men in mention:\n",
    "        mention_counter += 1\n",
    "        for t in men.tokens:\n",
    "            VOCABS.add(t)\n",
    "print(\"%d mention loaded\" % mention_counter)\n",
    "\n",
    "print(\"%d word loaded\" % len(VOCABS))\n",
    "\n",
    "w2vdict_file_path = \"./OntonoteNERCache/w2vdict.pkl\"\n",
    "w2vdict = load_cache_or_none(w2vdict_file_path)\n",
    "if w2vdict is None:\n",
    "    w2vdict=loadW2V(\"/home/haowu4/data/autoextend/GoogleNews-vectors-negative300.combined_500k.txt\", VOCABS)\n",
    "    save_cache(w2vdict, w2vdict_file_path)\n",
    "\n",
    "print(\"%d words have w2v\" % len(w2vdict))\n",
    "\n",
    "default_w2v_mean = np.mean(list(w2vdict.values()), axis=0)\n",
    "default_w2v_zero = np.zeros(default_w2v_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_mentions[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define features:\n",
    "\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# lmtzr = WordNetLemmatizer().lemmatize\n",
    "\n",
    "\n",
    "def word_shape_func(text):\n",
    "    text = re.sub(\"[a-z]+\", \"a\" ,text)\n",
    "    text = re.sub(\"[A-Z]+\", \"A\" ,text)\n",
    "    text = re.sub(\"[0-9]+\", \"0\" ,text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "class FeatureFunc(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, original_func):\n",
    "        decorator_self = self\n",
    "        def wrappee( *args, **kwargs):\n",
    "            for feat in original_func(*args,**kwargs):\n",
    "                yield (\"%s=%s\" % (unidecode(self.name), feat), 1.0) \n",
    "        return wrappee\n",
    "\n",
    "class RealFeatureFunc(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, original_func):\n",
    "        decorator_self = self\n",
    "        def wrappee( *args, **kwargs):\n",
    "            for feat,v in original_func(*args,**kwargs):\n",
    "                yield (\"%s=%s\" % (unidecode(self.name), feat), v)\n",
    "        return wrappee\n",
    "\n",
    "@FeatureFunc(\"dep_feature\")\n",
    "def mention_details(doc, start, end):\n",
    "    heads = [token.head for token in doc[start:end]]\n",
    "    deps = [list(token.children) for token in doc[start:end]]\n",
    "    for token, head, children in zip(doc[start:end], heads, deps):\n",
    "        if not (head.i >= start and head.i < end):            \n",
    "            yield \"<-%s- %s\" % (token.dep_, head.lemma_)\n",
    "            yield \"<- %s\" % (head.lemma_)\n",
    "        for child in children:\n",
    "            if not (child.i >= start and child.i < end):\n",
    "                yield  \"-%s-> %s\" % (child.dep_,child.lemma_)\n",
    "                yield  \"-> %s\" % (child.lemma_)\n",
    "\n",
    "def word_before(pos):\n",
    "    @FeatureFunc(\"word_before\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(max(start-pos,0), start):\n",
    "            yield doc[i].text\n",
    "#             yield word_shape_func(doc[i].text)\n",
    "    return f\n",
    "\n",
    "def word_before_loc(pos):\n",
    "    @FeatureFunc(\"word_before\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(max(start-pos,0), start):\n",
    "            yield \"%d-%s\" % (start - i,doc[i].text)\n",
    "    return f\n",
    "\n",
    "def word_before_lemma(pos):\n",
    "    @FeatureFunc(\"word_before_lemma\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(max(start-pos,0), start):\n",
    "            yield doc[i].lemma_\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_after(pos):\n",
    "    @FeatureFunc(\"word_after\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(end, min(end+pos,len(doc))):\n",
    "            yield doc[i].text\n",
    "#             yield word_shape_func(doc[i].text)\n",
    "    return f\n",
    "\n",
    "def word_after_loc(pos):\n",
    "    @FeatureFunc(\"word_after_loc\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(end, min(end+pos,len(doc))):\n",
    "            yield \"%d-%s\" % (i - end,doc[i].text)\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_after_lemma(pos):\n",
    "    @FeatureFunc(\"word_after_lemma\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(end, min(end+pos,len(doc))):\n",
    "            yield doc[i].lemma_\n",
    "        \n",
    "    return f\n",
    "\n",
    "@FeatureFunc(\"wim\")\n",
    "def word_in_mention(doc, start, end):\n",
    "    for x in doc[start:end]:\n",
    "        yield x.text\n",
    "        yield word_shape_func(x.text)\n",
    "\n",
    "@FeatureFunc(\"wim_lemma\")\n",
    "def word_in_mention_lemma(doc, start, end):\n",
    "    for x in doc[start:end]:\n",
    "        yield x.lemma_\n",
    "        \n",
    "\n",
    "@FeatureFunc(\"wim_loc\")\n",
    "def word_in_mention_loc(doc, start, end):\n",
    "    for i,x in enumerate(doc[start:end]):\n",
    "        yield \"f%d-%s\" % (i,x.text)\n",
    "        yield \"b%d-%s\" % ((end-start) - i,x.text)\n",
    "        \n",
    "@FeatureFunc(\"wim_loc_lemma\")\n",
    "def word_in_mention_loc_lemma(doc, start, end):\n",
    "    for i,x in enumerate(doc[start:end]):\n",
    "        x = x.lemma_\n",
    "        yield \"f%d-%s\" % (i,x)\n",
    "        yield \"b%d-%s\" % ((end-start)-i,x)\n",
    "\n",
    "\n",
    "\n",
    "# @FeatureFunc(\"wim_ext\")\n",
    "# def wim_ext(doc, start, end):\n",
    "#     for x in mention.ext_surface:\n",
    "#         yield x\n",
    "#         yield word_shape_func(x)\n",
    "    \n",
    "# @FeatureFunc(\"wim_ext_lemma\")\n",
    "# def wim_ext_lemma(doc, start, end):\n",
    "#     for x in mention.ext_surface:\n",
    "#         x = lmtzr(x)\n",
    "#         yield x\n",
    "    \n",
    "@FeatureFunc(\"wim_bigram\")\n",
    "def wim_bigram(doc, start, end):\n",
    "    for i,x in zip(doc[start:end-1], doc[start+1:end]):\n",
    "        yield \"%s-%s\" % (i.text,x.text)\n",
    "        \n",
    "@FeatureFunc(\"wim_bigram_lemma\")\n",
    "def wim_bigram_lemma(doc, start, end):\n",
    "    lms = [x.lemma_ for x in doc[start:end]]\n",
    "    for i,x in zip(lms[:-1], lms[1:]):\n",
    "        yield \"%s-%s\" % (i,x)\n",
    "\n",
    "\n",
    "        \n",
    "@FeatureFunc(\"word_shape\")\n",
    "def word_shape(doc, start, end):\n",
    "    t = \" \".join([x.text for x in doc[start:end]])\n",
    "    return [word_shape_func(t)]\n",
    "        \n",
    "@FeatureFunc(\"length\")\n",
    "def mention_length(doc, start, end):\n",
    "    return [\"%d\" % (end-start)]\n",
    "        \n",
    "@FeatureFunc(\"prefix\")\n",
    "def prefix(doc, start, end):\n",
    "    for w in doc[start:end]:\n",
    "        for i in range(3, min(5, len(w.text))):\n",
    "            yield w.text[:i]\n",
    "        \n",
    "@FeatureFunc(\"surfix\")\n",
    "def postfix(doc, start, end):\n",
    "    for w in doc[start:end]:\n",
    "        for i in range(3, min(5, len(w.text))):\n",
    "            yield w.text[-i:]\n",
    "\n",
    "@FeatureFunc(\"bias\")\n",
    "def CONSTANT_BIAS(doc, start, end):\n",
    "    return [\"bias\"]\n",
    "\n",
    "\n",
    "def kb_bias(kba):\n",
    "    @RealFeatureFunc(\"KBBias\")\n",
    "    def wrapee(doc, start, end):\n",
    "        surface = doc[start:end].text\n",
    "        try:\n",
    "            m = kba.surface_totype_dist[surface]\n",
    "        except KeyError:\n",
    "            return\n",
    "        ss = defaultdict(float)\n",
    "        for k in m:\n",
    "            if \".\" in k:\n",
    "                nk = k.split(\".\")[0]\n",
    "                ss[nk] += m[k]\n",
    "            else:\n",
    "                ss[k] += m[k]\n",
    "        for k in ss:\n",
    "            s = ss[k]\n",
    "            if s > 0.6:\n",
    "                yield (k, 1)\n",
    "    return wrapee\n",
    "# def gazzarteer(gas):\n",
    "#     @FeatureFunc(\"gazzarteer\")\n",
    "#     def gazzarteer_wrappee(mention):\n",
    "#         for gz_name in gas:\n",
    "#             if mention.surface:\n",
    "#                 return\n",
    "\n",
    "def getOrDefault(m, k, d):\n",
    "    if k in m:\n",
    "        return m[k]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "def w2vBefore(ws, default_w2v, pos = 4):    \n",
    "    def wrappee(mention):\n",
    "        words = []\n",
    "        for i in range(mention.start-pos, mention.start):\n",
    "            if i < 0:\n",
    "                words.append(np.zeros(default_w2v.shape))\n",
    "            else:\n",
    "                words.append(getOrDefault(ws,mention.tokens[i],default_w2v))\n",
    "        words.append(np.mean(words, axis=0))\n",
    "        return np.hstack(words)\n",
    "    return wrappee\n",
    "\n",
    "def w2vAfrer(ws, default_w2v, pos = 4):    \n",
    "    def wrappee(mention):\n",
    "        words = []\n",
    "        for i in range(mention.end, mention.end + pos):\n",
    "            if i >= len(mention.tokens):\n",
    "                words.append(np.zeros(default_w2v.shape))\n",
    "            else:\n",
    "                words.append(getOrDefault(ws,mention.tokens[i],default_w2v))\n",
    "        words.append(np.mean(words, axis=0))\n",
    "        return np.hstack(words)\n",
    "\n",
    "    return wrappee\n",
    "\n",
    "def w2vMention(ws, default_w2v):    \n",
    "    def wrappee(mention):\n",
    "        if len(mention.surface) == 0:\n",
    "            print(mention)\n",
    "        ms = [getOrDefault(ws,w,default_w2v) for w in mention.surface]\n",
    "        return np.mean(ms, axis=0)\n",
    "    return wrappee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_PATH_LEN = 4\n",
    "UP = 1\n",
    "DOWN = 2\n",
    "\n",
    "def shortest_path((x, y)):\n",
    "    \"\"\" Returns the shortest dependency path from x to y\n",
    "    :param x: x token\n",
    "    :param y: y token\n",
    "    :return: the shortest dependency path from x to y\n",
    "    \"\"\"\n",
    "\n",
    "    x_token = x\n",
    "    y_token = y\n",
    "    if not isinstance(x_token, spacy.tokens.token.Token):\n",
    "        x_token = x_token.root\n",
    "    if not isinstance(y_token, spacy.tokens.token.Token):\n",
    "        y_token = y_token.root\n",
    "\n",
    "    # Get the path from the root to each of the tokens\n",
    "    hx = heads(x_token)\n",
    "    hy = heads(y_token)\n",
    "\n",
    "    # Get the lowest common head\n",
    "    i = -1\n",
    "    for i in xrange(min(len(hx), len(hy))):\n",
    "        if hx[i] is not hy[i]:\n",
    "            break\n",
    "\n",
    "    if i == -1:\n",
    "        lch_idx = 0\n",
    "        if len(hy) > 0:\n",
    "            lch = hy[lch_idx]\n",
    "        elif len(hx) > 0:\n",
    "            lch = hx[lch_idx]\n",
    "        else:\n",
    "            lch = None\n",
    "    elif hx[i] == hy[i]:\n",
    "        lch_idx = i\n",
    "        lch = hx[lch_idx]\n",
    "    else:\n",
    "        lch_idx = i-1\n",
    "        lch = hx[lch_idx]\n",
    "\n",
    "    # The path from x to the lowest common head\n",
    "    hx = hx[lch_idx+1:]\n",
    "    if lch and check_direction(lch, hx, lambda h: h.lefts):\n",
    "        return None\n",
    "    hx = hx[::-1]\n",
    "\n",
    "    # The path from the lowest common head to y\n",
    "    hy = hy[lch_idx+1:]\n",
    "    if lch and check_direction(lch, hy, lambda h: h.rights):\n",
    "        return None\n",
    "\n",
    "    return (x, hx, lch, hy, y)\n",
    "\n",
    "\n",
    "def shortest_path2((x, y)):\n",
    "    \"\"\" Returns the shortest dependency path from x to y\n",
    "    :param x: x token\n",
    "    :param y: y token\n",
    "    :return: the shortest dependency path from x to y\n",
    "    \"\"\"\n",
    "\n",
    "    x_token = x\n",
    "    y_token = y\n",
    "    if not isinstance(x_token, spacy.tokens.token.Token):\n",
    "        x_token = x_token.root\n",
    "    if not isinstance(y_token, spacy.tokens.token.Token):\n",
    "        y_token = y_token.root\n",
    "\n",
    "    # Get the path from the root to each of the tokens including the tokens\n",
    "    hx = heads(x_token) + [x_token]\n",
    "    hy = heads(y_token) + [y_token]\n",
    "\n",
    "    # Get the lowest common head\n",
    "    i = -1\n",
    "    for i in xrange(min(len(hx), len(hy))):\n",
    "        if hx[i] is not hy[i]:\n",
    "            break\n",
    "\n",
    "    # i cannot be -1 since the path should atleast have the ROOT as the common ancestor\n",
    "    if hx[i] == hy[i]:\n",
    "        lch_idx = i\n",
    "        lch = hx[lch_idx]\n",
    "    else:\n",
    "        lch_idx = i-1\n",
    "        lch = hx[lch_idx]\n",
    "\n",
    "    # The path from x to the lowest common head\n",
    "    hx = hx[lch_idx+1:-1]\n",
    "    hx = hx[::-1]\n",
    "\n",
    "    # The path from the lowest common head to y\n",
    "    hy = hy[lch_idx+1:-1]\n",
    "\n",
    "    return (x, hx, lch, hy, y)\n",
    "\n",
    "\n",
    "def heads(token):\n",
    "    \"\"\"\n",
    "    Return the heads of a token, from the root down to immediate head\n",
    "    :param token:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    t = token\n",
    "    hs = []\n",
    "    while t is not t.head:\n",
    "        t = t.head\n",
    "        hs.append(t)\n",
    "    return hs[::-1]\n",
    "\n",
    "\n",
    "def direction(dir):\n",
    "    \"\"\"\n",
    "    Print the direction of the edge\n",
    "    :param dir: the direction\n",
    "    :return: a string representation of the direction\n",
    "    \"\"\"\n",
    "    # Up to the head\n",
    "    if dir == UP:\n",
    "        return '>'\n",
    "    # Down from the head\n",
    "    elif dir == DOWN:\n",
    "        return '<'\n",
    "\n",
    "\n",
    "def token_to_string(token):\n",
    "    \"\"\"\n",
    "    Convert the token to string representation\n",
    "    :param token:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not isinstance(token, spacy.tokens.token.Token):\n",
    "        return ' '.join([t.string.strip().lower() for t in token])\n",
    "    else:\n",
    "        return token.string.strip().lower()\n",
    "\n",
    "\n",
    "def token_to_lemma(token):\n",
    "    \"\"\"\n",
    "    Convert the token to string representation\n",
    "    :param token: the token\n",
    "    :return: string representation of the token\n",
    "    \"\"\"\n",
    "    if not isinstance(token, spacy.tokens.token.Token):\n",
    "        return token_to_string(token)\n",
    "    else:\n",
    "        return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "def clean_path((x, hx, lch, hy, y), entity_on_left=True, include_target_pos=False):\n",
    "    \"\"\"\n",
    "    Filter out long paths and pretty print the short ones\n",
    "    :return: the string representation of the path\n",
    "    \"\"\"\n",
    "    \n",
    "    def argument_to_string(token, edge_name, include_pos=False):\n",
    "        \"\"\"\n",
    "        Converts the argument token (X or Y) to an edge string representation\n",
    "        :param token: the X or Y token\n",
    "        :param edge_name: 'X' or 'Y'\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not isinstance(token, spacy.tokens.token.Token):\n",
    "            token = token.root\n",
    "\n",
    "        if include_pos:\n",
    "            return '/'.join([edge_name, token.pos_, token.dep_ if token.dep_ != '' else 'ROOT'])\n",
    "        else:\n",
    "            return '/'.join([edge_name, token.dep_ if token.dep_ != '' else 'ROOT'])\n",
    "    \n",
    "    def edge_to_string(token, is_head=False, is_lexicalized=True):\n",
    "        \"\"\"\n",
    "        Converts the token to an edge string representation\n",
    "        :param token: the token\n",
    "        :return: the edge string\n",
    "        \"\"\"\n",
    "        t = token\n",
    "        if not isinstance(token, spacy.tokens.token.Token):\n",
    "            t = token.root\n",
    "\n",
    "        if is_lexicalized:\n",
    "            return '/'.join([token_to_lemma(token), t.dep_ if t.dep_ != '' and not is_head else 'ROOT'])\n",
    "        else:\n",
    "            return '/'.join([t.pos_, t.dep_ if t.dep_ != '' and not is_head else 'ROOT'])\n",
    "\n",
    "\n",
    "    lch_lex_lst = []\n",
    "    lch_pos_lst = []\n",
    "\n",
    "    # X is the head\n",
    "    if isinstance(x, spacy.tokens.token.Token) and lch == x:\n",
    "        dir_x = ''\n",
    "        dir_y = direction(DOWN)\n",
    "    # Y is the head\n",
    "    elif isinstance(y, spacy.tokens.token.Token) and lch == y:\n",
    "        dir_x = direction(UP)\n",
    "        dir_y = ''\n",
    "    # X and Y are not heads\n",
    "    else:\n",
    "        lch_lex_lst = [edge_to_string(lch, is_head=True, is_lexicalized=True)] if lch else []\n",
    "        lch_pos_lst = [edge_to_string(lch, is_head=True, is_lexicalized=False)] if lch else []\n",
    "        dir_x = direction(UP)\n",
    "        dir_y = direction(DOWN)\n",
    "\n",
    "    len_path = len(hx) + len(hy) + len(lch_lex_lst)\n",
    "\n",
    "    if len_path <= MAX_PATH_LEN:\n",
    "#     if True:\n",
    "        mid_lex_path = (\n",
    "            [edge_to_string(token, is_lexicalized=True) + direction(UP) for token in hx] +\n",
    "            lch_lex_lst + \n",
    "            [direction(DOWN) + edge_to_string(token, is_lexicalized=True) for token in hy]\n",
    "        )\n",
    "\n",
    "        mid_pos_path = (\n",
    "            [edge_to_string(token, is_lexicalized=False) + direction(UP) for token in hx] + \n",
    "            lch_pos_lst + \n",
    "            [direction(DOWN) + edge_to_string(token, is_lexicalized=False) for token in hy]\n",
    "        )\n",
    "\n",
    "        if entity_on_left:\n",
    "            yield '_'.join(['ENT' + dir_x] + mid_lex_path + \n",
    "                           [dir_y + argument_to_string(y, y.lemma_, include_pos=include_target_pos)])\n",
    "            yield '_'.join(['ENT' + dir_x] + mid_pos_path + \n",
    "                           [dir_y + argument_to_string(y, y.lemma_, include_pos=include_target_pos)])\n",
    "        else:\n",
    "            yield '_'.join([argument_to_string(x, x.lemma_, include_pos=include_target_pos) + dir_x] + \n",
    "                           mid_lex_path + [dir_y + 'ENT'])\n",
    "            yield '_'.join([argument_to_string(x, x.lemma_, include_pos=include_target_pos) + dir_x] + \n",
    "                           mid_pos_path + [dir_y + 'ENT'])\n",
    "    else:\n",
    "        return\n",
    "        yield\n",
    "\n",
    "@FeatureFunc(\"prp_wh_dep_feat\")\n",
    "def mention_pronoun_wh_dep(doc, start, end):\n",
    "    PRP_SYM = nlp.vocab.strings[\"PRP\"]\n",
    "    WH_SYM = nlp.vocab.strings[\"WP\"]\n",
    "    prp_tokens = [token for token in doc if token.tag == PRP_SYM]\n",
    "    wh_tokens = [token for token in doc if token.tag == WH_SYM]\n",
    "    for mention_token in doc[start:end]:\n",
    "        for target_token in prp_tokens + wh_tokens:\n",
    "            path = shortest_path2((mention_token, target_token))\n",
    "            for cleaned_path in clean_path(path):\n",
    "                yield cleaned_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list(mention_pronoun_wh_dep(ex.doc, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "config = get_default_config()\n",
    "kbann = KBBiasTypeAnnotator(config, \"OntonoteType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating train vectors\n",
      "(140706, 34674)\n",
      "('dense_vecs[0].shape', (300,))\n",
      "('dense_vecs.shape', (140706, 300))\n",
      "('shapes : ', (140706, 34674), (140706, 300))\n",
      "generating test vectors\n",
      "(17085, 34674)\n",
      "('dense_vecs[0].shape', (300,))\n",
      "('dense_vecs.shape', (17085, 300))\n",
      "('shapes : ', (17085, 34674), (17085, 300))\n",
      "(579, 34674)\n",
      "('dense_vecs[0].shape', (300,))\n",
      "('dense_vecs.shape', (579, 300))\n",
      "('shapes : ', (579, 34674), (579, 300))\n"
     ]
    }
   ],
   "source": [
    "features = [CONSTANT_BIAS,\n",
    "            mention_details,\n",
    "            kb_bias(kbann),\n",
    "#             word_before(3), word_before_lemma(3),\n",
    "#             word_after(3), word_after_lemma(3),\n",
    "            \n",
    "            word_in_mention, word_in_mention_lemma,\n",
    "            word_in_mention_loc, word_in_mention_loc_lemma,\n",
    "            wim_bigram, wim_bigram_lemma,\n",
    "            \n",
    "#             wim_ext, wim_ext_lemma,\n",
    "            \n",
    "#             word_shape,\n",
    "            \n",
    "#             mention_length,\n",
    "            \n",
    "#             prefix,\n",
    "#             postfix,\n",
    "            \n",
    "#             mention_pronoun_wh_dep\n",
    "           ]\n",
    "\n",
    "default_w2v = default_w2v_mean\n",
    "\n",
    "dense_feature = [ #w2vBefore(w2vdict, default_w2v),\n",
    "                  #w2vAfrer(w2vdict, default_w2v),\n",
    "                  w2vMention(w2vdict, default_w2v)]\n",
    "\n",
    "def typ_func(m):\n",
    "    return m.label\n",
    "\n",
    "print(\"generating train vectors\")\n",
    "lex, type_lex, xs_train, ys_train = generate_vecs(filter(lambda mention: len(mention.tokens)>=5,train_mentions),\n",
    "                                      typ_func,\n",
    "                                      features,\n",
    "                                      dense_feature)\n",
    "\n",
    "print(\"generating test vectors\")\n",
    "_,_, xs_test, ys_test = generate_vecs(test_mentions,\n",
    "                            typ_func,\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)\n",
    "\n",
    "# _,_, xs_dev, ys_dev = generate_vecs(dev_mentions,\n",
    "#                             typ_func,\n",
    "#                             features,\n",
    "#                             dense_feature,\n",
    "#                             lex,\n",
    "#                             type_lex)\n",
    "\n",
    "_,_, xs_figer, _ = generate_vecs(figer_datas,\n",
    "                            lambda m : \"LOC\",\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn import linear_model, datasets, svm, ensemble\n",
    "\n",
    "# logreg = ensemble.BaggingClassifier(base_estimator=linear_model.Perceptron(penalty=\"l2\", alpha=1e-6), n_estimators=10, max_samples=.5)\n",
    "logreg = linear_model.SGDClassifier(loss = \"perceptron\",\n",
    "                                    penalty=\"l2\",\n",
    "                                    alpha=1e-6,\n",
    "                                    class_weight = \"balanced\",\n",
    "                                    average= True)\n",
    "logreg.fit(xs_train, ys_train)\n",
    "y_pred = logreg.predict(xs_test)\n",
    "y_pred_figer = logreg.predict(xs_figer)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933860111209\n"
     ]
    }
   ],
   "source": [
    "def fast_gen_vec(m):\n",
    "    _,_, e, _ = generate_vecs([m],\n",
    "                            lambda m : \"LOC\",\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)\n",
    "    return e\n",
    "\n",
    "def generate_example(tokens):\n",
    "    m = Mention(0, len(tokens), \"\", tokens)\n",
    "    _,_, e, _ = generate_vecs([m],\n",
    "                            lambda m : \"LOC\",\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)\n",
    "    return e\n",
    "\n",
    "# print(\"Binary f1  %.3f\" % (f1_score(ys_test, y_pred)))\n",
    "\n",
    "f1 = f1_score(ys_test, y_pred, average=\"micro\").tolist()\n",
    "print(f1)\n",
    "rd = {type_lex.m[x]:x for x in type_lex.m}\n",
    "\n",
    "class_names = [None] * len(rd)\n",
    "for i in range(len(rd)):\n",
    "    class_names[i] = rd[i]\n",
    "\n",
    "# for k,v in sorted([(rd[k], f1[k]) for k in rd], key=lambda a:a[1], reverse=True):\n",
    "#     print(\"%15s : %.3f\" % (k,v)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary f1  0.945\n",
    "[0.9561104805145667, 0.962999185004075, 0.9865470852017937, 0.9855574812247255, 0.928506513177825, 0.676470588235294, 0.9652351738241309, 0.9710485133020343, 0.9, 0.6827309236947792, 0.7031700288184438, 0.9870129870129871, 0.5, 0.9977081741787623, 0.9535353535353536, 0.5730659025787965, 0.7912087912087913, 0.6949152542372882]\n",
    "        PERCENT : 0.998\n",
    "          MONEY : 0.987\n",
    "        ORDINAL : 0.987\n",
    "           DATE : 0.986\n",
    "       CARDINAL : 0.971\n",
    "           NORP : 0.965\n",
    "            GPE : 0.963\n",
    "         PERSON : 0.956\n",
    "           TIME : 0.954\n",
    "            ORG : 0.929\n",
    "       QUANTITY : 0.900\n",
    "            LOC : 0.791\n",
    "    WORK_OF_ART : 0.703\n",
    "            LAW : 0.695\n",
    "          EVENT : 0.683\n",
    "        PRODUCT : 0.676\n",
    "            FAC : 0.573\n",
    "       LANGUAGE : 0.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i,m in enumerate(test_mentions):\n",
    "    if y_pred[i] == ys_test[i]:\n",
    "        continue\n",
    "    if m.label == \"FAC\":\n",
    "#         print(rd[y_pred[i]],m)\n",
    "        counter += 1\n",
    "    if counter == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "internet.website is missing\n",
      "internet.website is missing\n"
     ]
    }
   ],
   "source": [
    "matches =  defaultdict(int)\n",
    "alllabel_matches =  defaultdict(int)\n",
    "coarselabel_matches =  defaultdict(int)\n",
    "\n",
    "coarselabel_match_examples = defaultdict(list)\n",
    "\n",
    "match_examples =  defaultdict(list)\n",
    "all_labels = 0\n",
    "for i in range(len(figer_datas)):\n",
    "    preded = rd[y_pred_figer[i]]\n",
    "    stripped_type = strip_fine_type(figer_datas[i].label)\n",
    "    fine_types = sorted(figer_datas[i].label.split(\",\"))\n",
    "    if preded == \"NORP\":\n",
    "        pd_mapped = \"/norpl\"\n",
    "    else:\n",
    "        pd_mapped = \"/\" + getOrDefault(onto2figer_tm, preded, \"X\")\n",
    "\n",
    "    coarselabel_matches[(pd_mapped, stripped_type)] += 1    \n",
    "    coarselabel_match_examples[(pd_mapped, stripped_type)].append(figer_datas[i])\n",
    "    \n",
    "    alllabel_matches[(preded,\",\".join(fine_types))] += 1    \n",
    "    for l in fine_types:\n",
    "        all_labels += 1\n",
    "        matches[(preded, l)] += 1\n",
    "        match_examples[(preded, l)].append(figer_datas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ('/product', u'/organization')\t--\t         9\t--\t0.02\n",
      "                   ('/location', u'/organization')\t--\t         7\t--\t0.01\n",
      "                     ('/organization', u'/person')\t--\t         7\t--\t0.01\n",
      "                            ('/person', u'/title')\t--\t         6\t--\t0.01\n",
      "                      ('/organization', u'/title')\t--\t         6\t--\t0.01\n",
      "                   ('/organization', u'/location')\t--\t         6\t--\t0.01\n",
      "                         ('/location', u'/person')\t--\t         3\t--\t0.01\n",
      "                        ('/art', u'/organization')\t--\t         3\t--\t0.01\n",
      "                   ('/organization', u'/building')\t--\t         3\t--\t0.01\n",
      "                      ('/person', '/organization')\t--\t         3\t--\t0.01\n",
      "                                 ('/art', '/work')\t--\t         3\t--\t0.01\n",
      "                        ('/organization', '/work')\t--\t         3\t--\t0.01\n",
      "                      ('/organization', u'/event')\t--\t         3\t--\t0.01\n",
      "                   ('/building', u'/organization')\t--\t         3\t--\t0.01\n",
      "                               ('/X', '/medicine')\t--\t         2\t--\t0.00\n",
      "                              ('/person', '/work')\t--\t         2\t--\t0.00\n",
      "                        ('/law', u'/organization')\t--\t         2\t--\t0.00\n",
      "  ('/location', u'/location,/transportation.road')\t--\t         2\t--\t0.00\n",
      "                              ('/art', u'/person')\t--\t         2\t--\t0.00\n",
      "  ('/building', u'/location,/transportation.road')\t--\t         2\t--\t0.00\n",
      "                            ('/organization', 'O')\t--\t         1\t--\t0.00\n",
      "                    ('/organization', u'/product')\t--\t         1\t--\t0.00\n",
      "                               ('/X', u'/product')\t--\t         1\t--\t0.00\n",
      "                                   ('/X', '/work')\t--\t         1\t--\t0.00\n",
      "                                  ('/X', '/award')\t--\t         1\t--\t0.00\n",
      "                               ('/event', '/work')\t--\t         1\t--\t0.00\n",
      "                           ('/product', u'/event')\t--\t         1\t--\t0.00\n",
      "                   ('/location', u'/living_thing')\t--\t         1\t--\t0.00\n",
      "                                 ('/product', 'O')\t--\t         1\t--\t0.00\n",
      "                             ('/norpl', u'/title')\t--\t         1\t--\t0.00\n",
      "                          ('/person', '/medicine')\t--\t         1\t--\t0.00\n",
      "                        ('/art', u'/living_thing')\t--\t         1\t--\t0.00\n",
      "                            ('/person', u'/event')\t--\t         1\t--\t0.00\n",
      "                         ('/person', u'/location')\t--\t         1\t--\t0.00\n",
      "                            ('/norpl', u'/person')\t--\t         1\t--\t0.00\n",
      "                      ('/norpl', u'/living_thing')\t--\t         1\t--\t0.00\n",
      "                          ('/product', u'/person')\t--\t         1\t--\t0.00\n",
      "                              ('/person', u'/law')\t--\t         1\t--\t0.00\n",
      "0.164\n",
      "                           ('/person', u'/person')\t--\t       229\t--\t0.40\n",
      "                ('/organization', '/organization')\t--\t       142\t--\t0.25\n",
      "                       ('/location', u'/location')\t--\t        58\t--\t0.10\n",
      "                               ('/time', u'/time')\t--\t        26\t--\t0.04\n",
      "                        ('/building', '/building')\t--\t        13\t--\t0.02\n",
      "                              ('/norpl', '/norpl')\t--\t        10\t--\t0.02\n",
      "                             ('/event', u'/event')\t--\t         4\t--\t0.01\n",
      "                                 ('/law', u'/law')\t--\t         1\t--\t0.00\n",
      "                         ('/product', u'/product')\t--\t         1\t--\t0.00\n",
      "1.000\n"
     ]
    }
   ],
   "source": [
    "figer_len = float(len(figer_datas))\n",
    "\n",
    "err_pre = 0.0\n",
    "\n",
    "for k in sorted(coarselabel_matches.keys(), key=lambda x : coarselabel_matches[x], reverse=True):\n",
    "    a, b =k\n",
    "    if a != b:\n",
    "        err_pre += coarselabel_matches[k]/figer_len\n",
    "        print(\"%50s\\t--\\t%10d\\t--\\t%.2f\" % (k, coarselabel_matches[k], coarselabel_matches[k]/figer_len))\n",
    "print(\"%.3f\" % err_pre)\n",
    "\n",
    "\n",
    "for k in sorted(coarselabel_matches.keys(), key=lambda x : coarselabel_matches[x], reverse=True):\n",
    "    a, b =k\n",
    "    if a == b:\n",
    "        err_pre += coarselabel_matches[k]/figer_len\n",
    "        print(\"%50s\\t--\\t%10d\\t--\\t%.2f\" % (k, coarselabel_matches[k], coarselabel_matches[k]/figer_len))\n",
    "print(\"%.3f\" % err_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           (u'PERSON', u'/person')\t--\t       204\t--\t0.35\n",
      "                        (u'ORG', u'/organization')\t--\t        51\t--\t0.09\n",
      "(u'ORG', u'/organization,/organization/educational_institution')\t--\t        34\t--\t0.06\n",
      "             (u'GPE', u'/location,/location/city')\t--\t        32\t--\t0.06\n",
      "                               (u'DATE', u'/time')\t--\t        26\t--\t0.04\n",
      "  (u'ORG', u'/organization,/organization/company')\t--\t        15\t--\t0.03\n",
      "(u'ORG', u'/organization,/organization/sports_team')\t--\t        14\t--\t0.02\n",
      "                   (u'NORP', u'/people/ethnicity')\t--\t        10\t--\t0.02\n",
      "          (u'GPE', u'/location,/location/country')\t--\t        10\t--\t0.02\n",
      "           (u'PERSON', u'/person,/person/athlete')\t--\t         9\t--\t0.02\n",
      "     (u'ORG', u'/government_agency,/organization')\t--\t         8\t--\t0.01\n",
      "(u'ORG', u'/organization,/organization/sports_league')\t--\t         8\t--\t0.01\n",
      "         (u'GPE', u'/location,/location/province')\t--\t         8\t--\t0.01\n",
      "                              (u'ORG', u'/person')\t--\t         7\t--\t0.01\n",
      "                            (u'FAC', u'/building')\t--\t         7\t--\t0.01\n",
      "                               (u'ORG', u'/title')\t--\t         6\t--\t0.01\n",
      "                            (u'PERSON', u'/title')\t--\t         6\t--\t0.01\n",
      "(u'PRODUCT', u'/organization,/organization/sports_team')\t--\t         5\t--\t0.01\n",
      "        (u'PERSON', u'/person,/person/politician')\t--\t         5\t--\t0.01\n",
      "  (u'FAC', u'/building,/building/sports_facility')\t--\t         5\t--\t0.01\n",
      "             (u'ORG', u'/location,/location/city')\t--\t         5\t--\t0.01\n",
      "            (u'PERSON', u'/person,/person/artist')\t--\t         5\t--\t0.01\n",
      "(u'ORG', u'/news_agency,/organization,/organization/company')\t--\t         4\t--\t0.01\n",
      "                    (u'PRODUCT', u'/organization')\t--\t         4\t--\t0.01\n",
      "                             (u'EVENT', u'/event')\t--\t         4\t--\t0.01\n",
      "  (u'ORG', u'/education/department,/organization')\t--\t         4\t--\t0.01\n",
      "                            (u'LOC', u'/location')\t--\t         4\t--\t0.01\n",
      "           (u'PERSON', u'/person,/person/soldier')\t--\t         3\t--\t0.01\n",
      "                            (u'GPE', u'/location')\t--\t         3\t--\t0.01\n",
      "                (u'WORK_OF_ART', u'/organization')\t--\t         3\t--\t0.01\n",
      "                        (u'ORG', u'/written_work')\t--\t         3\t--\t0.01\n",
      "                         (u'WORK_OF_ART', u'/art')\t--\t         3\t--\t0.01\n",
      "(u'GPE', u'/organization,/organization/sports_team')\t--\t         3\t--\t0.01\n",
      "                               (u'ORG', u'/event')\t--\t         3\t--\t0.01\n",
      "              (u'ORG', u'/military,/organization')\t--\t         3\t--\t0.01\n",
      "                              (u'PERSON', u'/art')\t--\t         2\t--\t0.00\n",
      "             (u'PERSON', u'/person,/person/coach')\t--\t         2\t--\t0.00\n",
      "                        (u'FAC', u'/organization')\t--\t         2\t--\t0.00\n",
      "  (u'LAW', u'/education/department,/organization')\t--\t         2\t--\t0.00\n",
      "                        (u'LOC', u'/organization')\t--\t         2\t--\t0.00\n",
      "       (u'FAC', u'/location,/transportation/road')\t--\t         2\t--\t0.00\n",
      " (u'ORG', u'/government/government,/organization')\t--\t         1\t--\t0.00\n",
      "                            (u'ORG', u'/location')\t--\t         1\t--\t0.00\n",
      "                               (u'EVENT', u'/art')\t--\t         1\t--\t0.00\n",
      "(u'FAC', u'/organization,/organization/educational_institution')\t--\t         1\t--\t0.00\n",
      "                                 (u'LAW', u'/law')\t--\t         1\t--\t0.00\n",
      "      (u'ORG', u'/internet/website,/organization')\t--\t         1\t--\t0.00\n",
      "                              (u'PERSON', u'/law')\t--\t         1\t--\t0.00\n",
      "   (u'NORP', u'/living_thing,/livingthing/animal')\t--\t         1\t--\t0.00\n",
      "   (u'QUANTITY', u'/education/educational_degree')\t--\t         1\t--\t0.00\n",
      "                      (u'WORK_OF_ART', u'/person')\t--\t         1\t--\t0.00\n",
      "                              (u'LOC', u'/person')\t--\t         1\t--\t0.00\n",
      "(u'LOC', u'/organization,/organization/sports_team')\t--\t         1\t--\t0.00\n",
      "                            (u'PERSON', u'/event')\t--\t         1\t--\t0.00\n",
      "     (u'CARDINAL', u'/medicine/medical_treatment')\t--\t         1\t--\t0.00\n",
      "(u'WORK_OF_ART', u'/living_thing,/livingthing/animal')\t--\t         1\t--\t0.00\n",
      "                             (u'ORG', u'/product')\t--\t         1\t--\t0.00\n",
      "            (u'ORG', u'/building,/building/hotel')\t--\t         1\t--\t0.00\n",
      "                            (u'ORG', u'/building')\t--\t         1\t--\t0.00\n",
      "(u'PRODUCT', u'/internet/website,/organization,/organization/company')\t--\t         1\t--\t0.00\n",
      "             (u'PERSON', u'/organization/company')\t--\t         1\t--\t0.00\n",
      "                        (u'GPE', u'/organization')\t--\t         1\t--\t0.00\n",
      "         (u'FAC', u'/building,/building/hospital')\t--\t         1\t--\t0.00\n",
      "                   (u'CARDINAL', u'/written_work')\t--\t         1\t--\t0.00\n",
      "         (u'PRODUCT', u'/product,/product/camera')\t--\t         1\t--\t0.00\n",
      "            (u'PERSON', u'/person,/person/doctor')\t--\t         1\t--\t0.00\n",
      "             (u'LOC', u'/location,/location/city')\t--\t         1\t--\t0.00\n",
      "                             (u'NORP', u'/person')\t--\t         1\t--\t0.00\n",
      "                 (u'PERSON', u'/medicine/symptom')\t--\t         1\t--\t0.00\n",
      "       (u'GPE', u'/location,/transportation/road')\t--\t         1\t--\t0.00\n",
      "                              (u'GPE', u'/person')\t--\t         1\t--\t0.00\n",
      "                     (u'PERSON', u'/organization')\t--\t         1\t--\t0.00\n",
      "          (u'PERSON', u'/location,/location/city')\t--\t         1\t--\t0.00\n",
      "    (u'GPE', u'/living_thing,/livingthing/animal')\t--\t         1\t--\t0.00\n",
      "                        (u'CARDINAL', u'/product')\t--\t         1\t--\t0.00\n",
      "(u'PERSON', u'/organization,/organization/company')\t--\t         1\t--\t0.00\n",
      "                              (u'NORP', u'/title')\t--\t         1\t--\t0.00\n",
      "       (u'LOC', u'/location,/transportation/road')\t--\t         1\t--\t0.00\n",
      "                          (u'PRODUCT', u'/person')\t--\t         1\t--\t0.00\n",
      "               (u'QUANTITY', u'/medicine/symptom')\t--\t         1\t--\t0.00\n",
      "         (u'ORG', u'/building,/building/hospital')\t--\t         1\t--\t0.00\n",
      "              (u'LOC', u'/person,/person/athlete')\t--\t         1\t--\t0.00\n",
      "                           (u'PRODUCT', u'/event')\t--\t         1\t--\t0.00\n",
      "      (u'WORK_OF_ART', u'/person,/person/athlete')\t--\t         1\t--\t0.00\n"
     ]
    }
   ],
   "source": [
    "figer_len = float(len(figer_datas))\n",
    "for k in sorted(alllabel_matches.keys(), key=lambda x : alllabel_matches[x], reverse=True):\n",
    "    print(\"%50s\\t--\\t%10d\\t--\\t%.2f\" % (k, alllabel_matches[k], alllabel_matches[k]/figer_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_example(t):\n",
    "    return coarselabel_match_examples[t]\n",
    "kk=get_example((('ORG', '/person') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_revert_map_from_lex(lex):\n",
    "    r = [None] * len(lex.m)\n",
    "    for k in lex.m:\n",
    "        r[lex.m[k]] = k\n",
    "    return r\n",
    "\n",
    "rlex = generate_revert_map_from_lex(lex)\n",
    "\n",
    "def check_detail(example_, inv_type_lex, *kkks_):\n",
    "    xv_ = fast_gen_vec(example_)\n",
    "    print(\"\\n\\n\")\n",
    "    xv_ = np.asarray(xv_.todense()).flatten()\n",
    "    for kkk_ in kkks_:\n",
    "        print \"for label %s :\\n\" % inv_type_lex[kkk_]\n",
    "        prod_ = xv_ * logreg.coef_[kkk_]\n",
    "        print(\"Score : %.5f\\n\" % (np.sum(prod_) + logreg.intercept_[kkk_]))\n",
    "        inds = np.argsort(-np.abs(prod_))[:200]\n",
    "\n",
    "        weight_of_w2v = logreg.coef_[kkk_][len(lex.m):].reshape((1,300))\n",
    "        fweight_of_w2v = xv_[len(lex.m):].reshape((1,300))\n",
    "\n",
    "        w2v_scores = np.sum(weight_of_w2v * fweight_of_w2v, axis = 1).tolist()\n",
    "\n",
    "        words = []\n",
    "    #     for i in range(example_.start-4, example_.start):\n",
    "    #         if i < 0:\n",
    "    #             words.append(\"-*-\")\n",
    "    #         else:\n",
    "    #             words.append(example_.tokens[i])   \n",
    "\n",
    "    #     words.append(\"B-MEAN\")\n",
    "    #     for i in range(example_.end, example_.end + 4):\n",
    "    #         if i >= len(example_.tokens):\n",
    "    #             words.append(\"-*-\")\n",
    "    #         else:\n",
    "    #             words.append(example_.tokens[i])    \n",
    "\n",
    "    #     words.append(\"A-MEAN\")\n",
    "        words.append(\"MENTION-MEAN\")\n",
    "\n",
    "        for word, score in zip(words, w2v_scores):\n",
    "            print(\"%s --> %.3f\" %(word, score))\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        for ind in inds:\n",
    "            if ind >= len(rlex):\n",
    "                fname = \"W2V\"\n",
    "                continue\n",
    "            else:\n",
    "                fname = rlex[ind]\n",
    "            w_ = prod_[ind]\n",
    "            if w_ > 0:\n",
    "                print(\"%-20s : %.4f\" % (fname, w_) )\n",
    "        print( \"----\" * 10)\n",
    "        for ind in inds:\n",
    "            if ind >= len(rlex):\n",
    "                fname = \"W2V\"\n",
    "                continue\n",
    "            else:\n",
    "                fname = rlex[ind]\n",
    "            w_ = prod_[ind]\n",
    "            if w_ < 0:\n",
    "                print(\"%-20s : %.4f\" % (fname, w_) )\n",
    "        print \"\"\n",
    "        print \"=\"*60\n",
    "        print \"\"\n",
    "    print(example_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_revert_map_from_lex(lex):\n",
    "    r = [None] * len(lex.m)\n",
    "    for k in lex.m:\n",
    "        r[lex.m[k]] = k\n",
    "    return r\n",
    "\n",
    "rlex = generate_revert_map_from_lex(lex)\n",
    "\n",
    "def check_detail(example_, inv_type_lex, *kkks_):\n",
    "    xv_ = fast_gen_vec(example_)\n",
    "    print(\"\\n\\n\")\n",
    "    for kkk_ in kkks_:\n",
    "        print \"for label %s :\\n\" % inv_type_lex[kkk_]\n",
    "        prod_ = np.asarray(xv_.todense()).flatten() * logreg.coef_[kkk_]\n",
    "        inds = np.argsort(-np.abs(prod_))[:200]\n",
    "        for ind in inds:\n",
    "            if ind >= len(rlex):\n",
    "                fname = \"W2V\"\n",
    "                continue\n",
    "            else:\n",
    "                fname = rlex[ind]\n",
    "            w_ = prod_[ind]\n",
    "            if w_ > 0:\n",
    "                print(\"%-20s : %.4f\" % (fname, w_) )\n",
    "        print( \"----\" * 10)\n",
    "        for ind in inds:\n",
    "            if ind >= len(rlex):\n",
    "                fname = \"W2V\"\n",
    "                continue\n",
    "            else:\n",
    "                fname = rlex[ind]\n",
    "            w_ = prod_[ind]\n",
    "            if w_ < 0:\n",
    "                print(\"%-20s : %.4f\" % (fname, w_) )\n",
    "        print \"=\"*20\n",
    "        print \"\"\n",
    "    print(example_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'CARDINAL': 3,\n",
       " u'DATE': 5,\n",
       " u'EVENT': 15,\n",
       " u'FAC': 8,\n",
       " u'GPE': 1,\n",
       " u'LANGUAGE': 16,\n",
       " u'LAW': 17,\n",
       " u'LOC': 11,\n",
       " u'MONEY': 10,\n",
       " u'NORP': 2,\n",
       " u'ORDINAL': 7,\n",
       " u'ORG': 4,\n",
       " u'PERCENT': 13,\n",
       " u'PERSON': 0,\n",
       " u'PRODUCT': 9,\n",
       " u'QUANTITY': 12,\n",
       " u'TIME': 6,\n",
       " u'WORK_OF_ART': 14}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_lex.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "     \"start\": 22, \n",
       "     \"end\": 24, \n",
       "     \"surface\": [\n",
       "         \"Federal\", \n",
       "         \"Way\"\n",
       "     ], \n",
       "     \"label\": \"/location,/location/city\", \n",
       "     \"doc_id\": \"\", \n",
       "     \"tokens\": [\n",
       "         \"A\", \n",
       "         \"federal\", \n",
       "         \"grand\", \n",
       "         \"jury\", \n",
       "         \"has\", \n",
       "         \"indicted\", \n",
       "         \"eight\", \n",
       "         \"people\", \n",
       "         \"suspected\", \n",
       "         \"of\", \n",
       "         \"operating\", \n",
       "         \"a\", \n",
       "         \"human-trafficking\", \n",
       "         \"ring\", \n",
       "         \"for\", \n",
       "         \"interstate\", \n",
       "         \"prostitution\", \n",
       "         \"from\", \n",
       "         \"a\", \n",
       "         \"Korean\", \n",
       "         \"nightclub\", \n",
       "         \"in\", \n",
       "         \"Federal\", \n",
       "         \"Way\", \n",
       "         \".\"\n",
       "     ]\n",
       " }, {\n",
       "     \"start\": 11, \n",
       "     \"end\": 13, \n",
       "     \"surface\": [\n",
       "         \"Federal\", \n",
       "         \"Way\"\n",
       "     ], \n",
       "     \"label\": \"/location,/location/city\", \n",
       "     \"doc_id\": \"\", \n",
       "     \"tokens\": [\n",
       "         \"Also\", \n",
       "         \"arrested\", \n",
       "         \"Thursday\", \n",
       "         \"were\", \n",
       "         \":\", \n",
       "         \"Miyoung\", \n",
       "         \"Roberts\", \n",
       "         \",\", \n",
       "         \"40\", \n",
       "         \",\", \n",
       "         \"of\", \n",
       "         \"Federal\", \n",
       "         \"Way\", \n",
       "         \";\", \n",
       "         \"accused\", \n",
       "         \"of\", \n",
       "         \"being\", \n",
       "         \"the\", \n",
       "         \"current\", \n",
       "         \"madam\", \n",
       "         \"at\", \n",
       "         \"the\", \n",
       "         \"club\", \n",
       "         \".\"\n",
       "     ]\n",
       " }, {\n",
       "     \"start\": 6, \n",
       "     \"end\": 8, \n",
       "     \"surface\": [\n",
       "         \"Federal\", \n",
       "         \"Way\"\n",
       "     ], \n",
       "     \"label\": \"/location/city,/location\", \n",
       "     \"doc_id\": \"\", \n",
       "     \"tokens\": [\n",
       "         \"Raymond\", \n",
       "         \"Jung\", \n",
       "         \",\", \n",
       "         \"51\", \n",
       "         \",\", \n",
       "         \"of\", \n",
       "         \"Federal\", \n",
       "         \"Way\", \n",
       "         \";\", \n",
       "         \"accused\", \n",
       "         \"of\", \n",
       "         \"leasing\", \n",
       "         \"apartments\", \n",
       "         \"where\", \n",
       "         \"the\", \n",
       "         \"women\", \n",
       "         \"were\", \n",
       "         \"housed\", \n",
       "         \".\"\n",
       "     ]\n",
       " }, {\n",
       "     \"start\": 7, \n",
       "     \"end\": 9, \n",
       "     \"surface\": [\n",
       "         \"Federal\", \n",
       "         \"Way\"\n",
       "     ], \n",
       "     \"label\": \"/location/city,/location\", \n",
       "     \"doc_id\": \"\", \n",
       "     \"tokens\": [\n",
       "         \"Kwang\", \n",
       "         \"Frank\", \n",
       "         \"Lee\", \n",
       "         \",\", \n",
       "         \"57\", \n",
       "         \",\", \n",
       "         \"of\", \n",
       "         \"Federal\", \n",
       "         \"Way\", \n",
       "         \";\", \n",
       "         \"accused\", \n",
       "         \"of\", \n",
       "         \"providing\", \n",
       "         \"money\", \n",
       "         \"to\", \n",
       "         \"finance\", \n",
       "         \"a\", \n",
       "         \"bogus\", \n",
       "         \"marriage\", \n",
       "         \"tied\", \n",
       "         \"to\", \n",
       "         \"the\", \n",
       "         \"scheme\", \n",
       "         \".\"\n",
       "     ]\n",
       " }, {\n",
       "     \"start\": 16, \n",
       "     \"end\": 18, \n",
       "     \"surface\": [\n",
       "         \"Federal\", \n",
       "         \"Way\"\n",
       "     ], \n",
       "     \"label\": \"/location/city,/location\", \n",
       "     \"doc_id\": \"\", \n",
       "     \"tokens\": [\n",
       "         \"Two\", \n",
       "         \"other\", \n",
       "         \"defendants\", \n",
       "         \"\\u2013\", \n",
       "         \"Sung\", \n",
       "         \"Hee\", \n",
       "         \"Han\", \n",
       "         \"and\", \n",
       "         \"Hee\", \n",
       "         \"Jae\", \n",
       "         \"Cho\", \n",
       "         \",\", \n",
       "         \"both\", \n",
       "         \"40\", \n",
       "         \"and\", \n",
       "         \"of\", \n",
       "         \"Federal\", \n",
       "         \"Way\", \n",
       "         \"\\u2013\", \n",
       "         \"still\", \n",
       "         \"are\", \n",
       "         \"sought\", \n",
       "         \".\"\n",
       "     ]\n",
       " }, {\n",
       "     \"start\": 6, \n",
       "     \"end\": 8, \n",
       "     \"surface\": [\n",
       "         \"Bridger\", \n",
       "         \"Bowl\"\n",
       "     ], \n",
       "     \"label\": \"/location\", \n",
       "     \"doc_id\": \"\", \n",
       "     \"tokens\": [\n",
       "         \"However\", \n",
       "         \",\", \n",
       "         \"the\", \n",
       "         \"outage\", \n",
       "         \"shut\", \n",
       "         \"down\", \n",
       "         \"Bridger\", \n",
       "         \"Bowl\", \n",
       "         \"for\", \n",
       "         \"the\", \n",
       "         \"rest\", \n",
       "         \"of\", \n",
       "         \"the\", \n",
       "         \"day\", \n",
       "         \",\", \n",
       "         \"according\", \n",
       "         \"to\", \n",
       "         \"a\", \n",
       "         \"posting\", \n",
       "         \"on\", \n",
       "         \"the\", \n",
       "         \"ski\", \n",
       "         \"area\", \n",
       "         \"'s\", \n",
       "         \"Facebook\", \n",
       "         \"page\", \n",
       "         \".\"\n",
       "     ]\n",
       " }]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarselabel_match_examples[('/organization', '/location')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'CARDINAL': 3,\n",
       " u'DATE': 5,\n",
       " u'EVENT': 15,\n",
       " u'FAC': 8,\n",
       " u'GPE': 1,\n",
       " u'LANGUAGE': 16,\n",
       " u'LAW': 17,\n",
       " u'LOC': 11,\n",
       " u'MONEY': 10,\n",
       " u'NORP': 2,\n",
       " u'ORDINAL': 7,\n",
       " u'ORG': 4,\n",
       " u'PERCENT': 13,\n",
       " u'PERSON': 0,\n",
       " u'PRODUCT': 9,\n",
       " u'QUANTITY': 12,\n",
       " u'TIME': 6,\n",
       " u'WORK_OF_ART': 14}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_type_lex = {v:k for k, v in type_lex.m.iteritems()}\n",
    "type_lex.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 34674)\n",
      "('dense_vecs[0].shape', (300,))\n",
      "('dense_vecs.shape', (1, 300))\n",
      "('shapes : ', (1, 34674), (1, 300))\n",
      "\n",
      "\n",
      "\n",
      "for label LOC :\n",
      "\n",
      "dep_feature=<-pobj- of : 6.1723\n",
      "dep_feature=<- of    : 4.1910\n",
      "----------------------------------------\n",
      "bias=bias            : -60.3990\n",
      "KBBias=location      : -46.3647\n",
      "wim=Aa               : -29.7549\n",
      "wim=Federal          : -11.7027\n",
      "wim_lemma=federal    : -11.7027\n",
      "wim_loc=b2-Federal   : -3.0042\n",
      "wim_loc_lemma=b2-federal : -3.0042\n",
      "wim_loc_lemma=f0-federal : -1.3525\n",
      "wim_loc=f0-Federal   : -1.3525\n",
      "====================\n",
      "\n",
      "for label ORG :\n",
      "\n",
      "dep_feature=<-pobj- of : 4.6822\n",
      "wim=Federal          : 4.3280\n",
      "wim_lemma=federal    : 2.9289\n",
      "wim_loc_lemma=f0-federal : 2.8428\n",
      "wim_loc=f0-Federal   : 2.1542\n",
      "dep_feature=<- of    : 1.0424\n",
      "wim=Aa               : 0.9941\n",
      "----------------------------------------\n",
      "bias=bias            : -20.3572\n",
      "KBBias=location      : -14.8247\n",
      "wim_lemma=way        : -5.7287\n",
      "wim=Way              : -5.7287\n",
      "wim_loc=b2-Federal   : -0.3150\n",
      "wim_loc_lemma=b2-federal : -0.3150\n",
      "====================\n",
      "\n",
      "{\n",
      "    \"start\": 6, \n",
      "    \"end\": 8, \n",
      "    \"surface\": [\n",
      "        \"Federal\", \n",
      "        \"Way\"\n",
      "    ], \n",
      "    \"label\": \"/location/city,/location\", \n",
      "    \"doc_id\": \"\", \n",
      "    \"tokens\": [\n",
      "        \"Raymond\", \n",
      "        \"Jung\", \n",
      "        \",\", \n",
      "        \"51\", \n",
      "        \",\", \n",
      "        \"of\", \n",
      "        \"Federal\", \n",
      "        \"Way\", \n",
      "        \";\", \n",
      "        \"accused\", \n",
      "        \"of\", \n",
      "        \"leasing\", \n",
      "        \"apartments\", \n",
      "        \"where\", \n",
      "        \"the\", \n",
      "        \"women\", \n",
      "        \"were\", \n",
      "        \"housed\", \n",
      "        \".\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "check_detail(get_example((('/organization', u'/location')))[2], inv_type_lex, 11, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\" \".join(get_example(('ORG', '/person'))[11].tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex.counter_per_type[\"wim_loc=f0-Mackenzie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\" \".join(get_example(('ORG', '/person'))[11].tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex = get_example(('ORG', '/person'))[11]\n",
    "shortest_path2((ex.doc[0], ex.doc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heads(ex.doc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(clean_path((shortest_path2((ex.doc[2], ex.doc[0]))), entity_on_left=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shortest_path((ex.doc[0], ex.doc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex = get_example(('ORG', '/person'))[6]\n",
    "def pronoun_feat(doc, start, end):\n",
    "    PRP_SYM = nlp.vocab.strings[\"PRP\"]\n",
    "    prps = [token.i for token in doc if token.tag == PRP_SYM]\n",
    "    \n",
    "    print prps\n",
    "pronoun_feat(ex.doc, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter(lambda feat: feat.startswith(\"prp\"), lex.m.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_m = get_example(('ORG', '/person'))[1]\n",
    "for k in mention_details(_m.doc, _m.start, _m.end ):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_examples= get_example((('ORG', '/person') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(logreg, \"/home/haowu4/data/ontonotes_model/logreg.pkl\")\n",
    "with open(\"/home/haowu4/data/ontonotes_model/lex.pkl\", 'wb') as f_lex:\n",
    "    with open(\"/home/haowu4/data/ontonotes_model/type_lex.pkl\", 'wb') as f_type_lex:\n",
    "        pickle.dump(lex.m, f_lex, pickle.HIGHEST_PROTOCOL)\n",
    "        pickle.dump(type_lex.m, f_type_lex, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "# with open(\"/home/haowu4/data/ontonotes_model/lex.pkl\", 'r') as f_lex:\n",
    "#     with open(\"/home/haowu4/data/ontonotes_model/type_lex.pkl\", 'r') as f_type_lex:\n",
    "#         lex.m = pickle.load(f_lex)\n",
    "#         pickle.load(type_lex.m, f_type_lex, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x34950 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 304 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(xs_figer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"start\": 7, \n",
       "    \"end\": 10, \n",
       "    \"surface\": [\n",
       "        \"Department\", \n",
       "        \"of\", \n",
       "        \"Chemistry\"\n",
       "    ], \n",
       "    \"label\": \"/education/department,/organization\", \n",
       "    \"doc_id\": \"\", \n",
       "    \"tokens\": [\n",
       "        \"A\", \n",
       "        \"handful\", \n",
       "        \"of\", \n",
       "        \"professors\", \n",
       "        \"in\", \n",
       "        \"the\", \n",
       "        \"UW\", \n",
       "        \"Department\", \n",
       "        \"of\", \n",
       "        \"Chemistry\", \n",
       "        \"are\", \n",
       "        \"being\", \n",
       "        \"recognized\", \n",
       "        \"by\", \n",
       "        \"the\", \n",
       "        \"American\", \n",
       "        \"Association\", \n",
       "        \"for\", \n",
       "        \"the\", \n",
       "        \"Advancement\", \n",
       "        \"of\", \n",
       "        \"Science\", \n",
       "        \"(\", \n",
       "        \"AAAS\", \n",
       "        \")\", \n",
       "        \"for\", \n",
       "        \"their\", \n",
       "        \"efforts\", \n",
       "        \"and\", \n",
       "        \"contributions\", \n",
       "        \"to\", \n",
       "        \"the\", \n",
       "        \"scientific\", \n",
       "        \"community\", \n",
       "        \".\"\n",
       "    ]\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figer_datas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x34950 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 321 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_figer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579, 34650)\n",
      "('dense_vecs[0].shape', (300,))\n",
      "('dense_vecs.shape', (579, 300))\n",
      "('shapes : ', (579, 34650), (579, 300))\n"
     ]
    }
   ],
   "source": [
    "_,_, xs_figer, _ = generate_vecs(figer_datas,\n",
    "                            lambda m : \"LOC\",\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wim=Aa\n",
      "wim_bigram=Department-of\n",
      "wim=of\n",
      "wim_bigram_lemma=department-of\n",
      "wim_lemma=of\n",
      "wim_loc_lemma=b3-department\n",
      "wim=a\n",
      "wim_loc_lemma=b2-of\n",
      "wim_loc=b3-Department\n",
      "wim_loc=b2-of\n",
      "wim_lemma=department\n",
      "dep_feature=-det-> the\n",
      "wim_loc=f1-of\n",
      "wim_loc_lemma=f0-department\n",
      "bias=bias\n",
      "dep_feature=<- in\n",
      "wim_loc_lemma=f1-of\n",
      "wim_loc=f0-Department\n",
      "wim=Department\n",
      "dep_feature=-> the\n",
      "dep_feature=<-pobj- in\n"
     ]
    }
   ],
   "source": [
    "for v in xs_figer[1].nonzero()[1].tolist():\n",
    "    if v < len(lex.m):\n",
    "        print(rf_map[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_map = {lex.m[i]:i for i in lex.m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'wim_bigram_lemma=department-of'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_map[4297]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1386,  2604,  2754,  4297,  5062,  5110,  8417,  9292, 15042,\n",
       "       16190, 17257, 17805, 18688, 19246, 19955, 20837, 23396, 24856,\n",
       "       29344, 33264, 33476, 34650, 34651, 34652, 34653, 34654, 34655,\n",
       "       34656, 34657, 34658, 34659, 34660, 34661, 34662, 34663, 34664,\n",
       "       34665, 34666, 34667, 34668, 34669, 34670, 34671, 34672, 34673,\n",
       "       34674, 34675, 34676, 34677, 34678, 34679, 34680, 34681, 34682,\n",
       "       34683, 34684, 34685, 34686, 34687, 34688, 34689, 34690, 34691,\n",
       "       34692, 34693, 34694, 34695, 34696, 34697, 34698, 34699, 34700,\n",
       "       34701, 34702, 34703, 34704, 34705, 34706, 34707, 34708, 34709,\n",
       "       34710, 34711, 34712, 34713, 34714, 34715, 34716, 34717, 34718,\n",
       "       34719, 34720, 34721, 34722, 34723, 34724, 34725, 34726, 34727,\n",
       "       34728, 34729, 34730, 34731, 34732, 34733, 34734, 34735, 34736,\n",
       "       34737, 34738, 34739, 34740, 34741, 34742, 34743, 34744, 34745,\n",
       "       34746, 34747, 34748, 34749, 34750, 34751, 34752, 34753, 34754,\n",
       "       34755, 34756, 34757, 34758, 34759, 34760, 34761, 34762, 34763,\n",
       "       34764, 34765, 34766, 34767, 34768, 34769, 34770, 34771, 34772,\n",
       "       34773, 34774, 34775, 34776, 34777, 34778, 34779, 34780, 34781,\n",
       "       34782, 34783, 34784, 34785, 34786, 34787, 34788, 34789, 34790,\n",
       "       34791, 34792, 34793, 34794, 34795, 34796, 34797, 34798, 34799,\n",
       "       34800, 34801, 34802, 34803, 34804, 34805, 34806, 34807, 34808,\n",
       "       34809, 34810, 34811, 34812, 34813, 34814, 34815, 34816, 34817,\n",
       "       34818, 34819, 34820, 34821, 34822, 34823, 34824, 34825, 34826,\n",
       "       34827, 34828, 34829, 34830, 34831, 34832, 34833, 34834, 34835,\n",
       "       34836, 34837, 34838, 34839, 34840, 34841, 34842, 34843, 34844,\n",
       "       34845, 34846, 34847, 34848, 34849, 34850, 34851, 34852, 34853,\n",
       "       34854, 34855, 34856, 34857, 34858, 34859, 34860, 34861, 34862,\n",
       "       34863, 34864, 34865, 34866, 34867, 34868, 34869, 34870, 34871,\n",
       "       34872, 34873, 34874, 34875, 34876, 34877, 34878, 34879, 34880,\n",
       "       34881, 34882, 34883, 34884, 34885, 34886, 34887, 34888, 34889,\n",
       "       34890, 34891, 34892, 34893, 34894, 34895, 34896, 34897, 34898,\n",
       "       34899, 34900, 34901, 34902, 34903, 34904, 34905, 34906, 34907,\n",
       "       34908, 34909, 34910, 34911, 34912, 34913, 34914, 34915, 34916,\n",
       "       34917, 34918, 34919, 34920, 34921, 34922, 34923, 34924, 34925,\n",
       "       34926, 34927, 34928, 34929, 34930, 34931, 34932, 34933, 34934,\n",
       "       34935, 34936, 34937, 34938, 34939, 34940, 34941, 34942, 34943,\n",
       "       34944, 34945, 34946, 34947, 34948, 34949], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_figer[1].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-13.32179681, -25.25818249, -14.91354345, -22.44403821,\n",
       "        13.14998444, -23.47352767, -30.6696133 ,  -6.89452493,\n",
       "       -18.32095584, -16.20493598,  -7.23660526, -21.13938825,\n",
       "       -26.65806334, -10.19096394, -31.52200585, -43.80509653,\n",
       "       -22.84706278, -53.22492515])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(logreg.predict(xs_figer[0]))\n",
    "logreg.decision_function(xs_figer[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_allocate_parameter_mem',\n",
       " '_estimator_type',\n",
       " '_expanded_class_weight',\n",
       " '_fit',\n",
       " '_fit_binary',\n",
       " '_fit_multiclass',\n",
       " '_get_learning_rate_type',\n",
       " '_get_loss_function',\n",
       " '_get_param_names',\n",
       " '_get_penalty_type',\n",
       " '_partial_fit',\n",
       " '_predict_proba_lr',\n",
       " '_validate_params',\n",
       " '_validate_sample_weight',\n",
       " 'alpha',\n",
       " 'average',\n",
       " 'class_weight',\n",
       " 'classes_',\n",
       " 'coef_',\n",
       " 'decision_function',\n",
       " 'densify',\n",
       " 'epsilon',\n",
       " 'eta0',\n",
       " 'fit',\n",
       " 'fit_intercept',\n",
       " 'fit_transform',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'l1_ratio',\n",
       " 'learning_rate',\n",
       " 'loss',\n",
       " 'loss_function',\n",
       " 'loss_functions',\n",
       " 'n_iter',\n",
       " 'n_jobs',\n",
       " 'partial_fit',\n",
       " 'penalty',\n",
       " 'power_t',\n",
       " 'predict',\n",
       " 'random_state',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'shuffle',\n",
       " 'sparsify',\n",
       " 't_',\n",
       " 'transform',\n",
       " 'verbose',\n",
       " 'warm_start']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from dfiner.types.finer_type_system import FinerTypeSystem\n",
    "from dfiner.utils import get_default_config\n",
    "config = get_default_config()\n",
    "\n",
    "tps = FinerTypeSystem.load_type_system(config)\n",
    "\n",
    "def strip_fine_type(label):\n",
    "    labels = label.split(\",\")\n",
    "    coarse_types = set()\n",
    "    # print(\"-------------\")\n",
    "    for label in labels:\n",
    "        label = label.replace(\"/\", \".\")[1:]\n",
    "        if not tps.has_type(label):\n",
    "            print >> sys.stderr, \"%s is missing\" % label\n",
    "            return \"O\"\n",
    "        else:\n",
    "            label = tps.get_root(label)\n",
    "        coarse_types.add(\"/%s\" % label)\n",
    "    # print(\"-------------\")\n",
    "    return \",\".join(sorted(list(coarse_types)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"/home/haowu4/codes/dataless_finer/resources/ontonote_to_figer_type.yaml\") as input:\n",
    "    onto2figer_tm = yaml.load(input.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getOrDefault(onto2figer_tm, \"QUANTITY\", \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people.ethnicity'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto2figer_tm[\"NORP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onto2figer_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Supersonics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-f1c5221c3d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkbann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurface_totype_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Supersonics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Supersonics'"
     ]
    }
   ],
   "source": [
    "kbann.surface_totype_dist[\"Supersonics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dataless_finer]",
   "language": "python",
   "name": "conda-env-dataless_finer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
