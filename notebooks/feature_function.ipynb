{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dfiner.classifier.lexicon import Lexicon\n",
    "from dfiner.utils.utils import dump_pickle, load_pickle\n",
    "import os\n",
    "import types\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "class FeatureStroage(object):\n",
    "    \n",
    "    cache_dir = \"/tmp/cache\"\n",
    "    COO = \"COO\"\n",
    "    DENSE = \"DENSE\"\n",
    "\n",
    "    @staticmethod\n",
    "    def save_sparse_csr(filename,array):\n",
    "        np.savez(filename,\n",
    "                 data=array.data,\n",
    "                 row=array.row,\n",
    "                 col=array.col, \n",
    "                 shape=array.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_sparse_csr(mat_file):\n",
    "        mat_file = \"%.coo_mat\" % mat_file        \n",
    "        loader = np.load(filename)\n",
    "        data = loader['data']\n",
    "        row = loader['row']\n",
    "        col = loader['col']\n",
    "        shape = loader['shape']\n",
    "        return coo_matrix((data, (row, col)), shape=shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_mat(mat, filename):\n",
    "        # We assume the input is either plain dense numpy matrix,\n",
    "        # or coo_matrix from scipy\n",
    "        if sparse.isspmatrix_coo(mat):\n",
    "            with open(filename, \"wb\") as out:\n",
    "                np.save(mat, out)\n",
    "    \n",
    "        else:\n",
    "            with open(filename, \"wb\") as out:\n",
    "                save_sparse_csr(out, mat)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cache_exist(mat_file):\n",
    "        if os.path.exists(\"%.dense_mat\" % mat_path):\n",
    "            return FeatureStroage.DENSE\n",
    "        else:\n",
    "            if os.path.exists(\"%.coo_mat\" % mat_path):\n",
    "                return FeatureStroage.COO\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    @classmethod\n",
    "    def load(feature_name, corpora_name):\n",
    "        mat_path = os.path.join(cache_dir,\n",
    "                                \"%s__%s.cache_mat\" % (feature_name,\n",
    "                                                      corpora_name))\n",
    "        existed = cache_exist(mat_path)\n",
    "        if not existed:\n",
    "            return None\n",
    "        else:\n",
    "            # Do extraction.\n",
    "            if existed is FeatureStroage.COO:                \n",
    "                return FeatureStroage.load_sparse_csr(mat_file)\n",
    "            \n",
    "            if existed is FeatureStroage.DENSE:\n",
    "                return FeatureStroage.load_dense_mat(mat_file)\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    def __init__(self, feature_functions):\n",
    "        self.ffs = feature_functions\n",
    "    \n",
    "    def extract(self, corpora_name, corpora, force_update=True , save_new_to_cache = True):\n",
    "        matrices = []\n",
    "        lookup_lexicons = []\n",
    "        for feature_func in self.ffs:\n",
    "            cache = None\n",
    "            if not force_update:\n",
    "                # Allow to use cache\n",
    "                cache = FeatureStroage.load(feature_func, corpora_name)\n",
    "                if cache:\n",
    "                    # Cache is valid\n",
    "                    mat = cache\n",
    "                    matrices.append(mat)\n",
    "            \n",
    "            if not cache:           \n",
    "                # If not allow to use cache or cache missed.\n",
    "                mat = feature_func.extrcat(corpora)\n",
    "                matrices.append(mat)\n",
    "            \n",
    "            for v in feature_func.lex:\n",
    "                lookup_lexicons.append(v)\n",
    "        return (matrices, lookup_lexicons)\n",
    "                        \n",
    "    def build_lexicon(self, corpora_name, corpora, min_support=5, force_update=False):\n",
    "        for feature_func in self.ffs:            \n",
    "            feature_func.build_lexicon(corpora, min_support, force_update)\n",
    "            \n",
    "            \n",
    "class FeatureFunction_(object):\n",
    "    \n",
    "    cache_dir = \"/tmp/cache\"\n",
    "    \n",
    "    def __init__(self, feature_func, name, reuse_lex_from_cache = True):\n",
    "        self.lex = None\n",
    "        self.func = feature_func\n",
    "        self.name = name\n",
    "        if reuse_lex_from_cache:\n",
    "            lex_path = os.path.join(FeatureFunction_.cache_dir,\n",
    "                            \"%s.cache_lex\" % (name))\n",
    "            if os.path.exists(lex_path):\n",
    "                ## Load the lex file\n",
    "                self.lex = load_pickle(lex_path)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"%s.FeatureFunction()\" % self.name\n",
    "\n",
    "    def build_lexicon(self, corpora, min_support=5, force_update=False):\n",
    "        if not force_update and self.lex is not None:\n",
    "            return\n",
    "        self.lex = Lexicon()\n",
    "        \n",
    "        \n",
    "    def freeze_lexicon(self, lex):\n",
    "        self.lex.allow_new_lexemes = False\n",
    "    \n",
    "    def prune(self, min_support):\n",
    "        self.lex.prune(min_support)\n",
    "        \n",
    "    def matrix_of(self, objs):\n",
    "        if self.lex is None:\n",
    "            self.lex = Lexicon()\n",
    "        features = []\n",
    "        for x in objs:\n",
    "            ret = self.func(x)\n",
    "            row = []\n",
    "            for x in ret:\n",
    "                try:\n",
    "                    k, v = x\n",
    "                except ValueError:\n",
    "                    k = x\n",
    "                    v = 1.0\n",
    "                row.append((k,v))\n",
    "            features.append(row)\n",
    "        return features\n",
    " \n",
    "            \n",
    "class DenseFeatureFunction_(object):\n",
    "        \n",
    "    def __init__(self, feature_func, name):\n",
    "        self.lex = None\n",
    "        self.func = feature_func\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"%s.FeatureFunction()\" % self.name\n",
    "        \n",
    "    def freeze_lexicon(self, lex):\n",
    "        self.lex.allow_new_lexemes = False\n",
    "    \n",
    "    def prune(self, min_support):\n",
    "        self.lex.prune(min_support)\n",
    "        \n",
    "    def matrix_of(self, objs):\n",
    "        rows = []\n",
    "        for x in objs:\n",
    "            row = self.func(x)\n",
    "            rows.append(row)\n",
    "        return np.vstack(rows)\n",
    "    \n",
    "class FeatureFunction(object):\n",
    "    def __init__(self, name = None, reuse_lex = True):\n",
    "        self.name = name\n",
    "        self.reuse_lex = reuse_lex\n",
    "\n",
    "    def __call__(self, original_func):\n",
    "        \n",
    "        if self.name is None:\n",
    "            self.name = original_func.__name__\n",
    "        \n",
    "        return FeatureFunction_(original_func, self.name, self.reuse_lex)\n",
    "    \n",
    "class DenseFeatureFunction(object):\n",
    "    def __init__(self, name = None):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, original_func):\n",
    "        \n",
    "        if self.name is None:\n",
    "            self.name = original_func.__name__\n",
    "        \n",
    "        return DenseFeatureFunction_(original_func, self.name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_when_run = True\n",
    "@FeatureFunction()\n",
    "def length(s):\n",
    "    if print_when_run:\n",
    "        print(\"length running...\")\n",
    "    yield \"%d\" % len(s)\n",
    "    \n",
    "@FeatureFunction()\n",
    "def letter_in_word(s):\n",
    "    if print_when_run:\n",
    "        print(\"letter_in_word running...\")\n",
    "    for c in s:\n",
    "        yield c\n",
    "\n",
    "@FeatureFunction()\n",
    "def letter_freq_in_word(s):\n",
    "    if print_when_run:\n",
    "        print(\"letter_in_word running...\")\n",
    "    m = defaultdict(int)\n",
    "    for c in s:\n",
    "        m[c] += 1.0\n",
    "    for c in m:\n",
    "        yield c, m[c]\n",
    "\n",
    "        \n",
    "@DenseFeatureFunction()\n",
    "def number_of_a_and_b(s):\n",
    "    if print_when_run:\n",
    "        print(\"number_of_a_and_b running...\")    \n",
    "    ac = 0\n",
    "    bc = 0\n",
    "    for w in s:\n",
    "        if w == \"a\":\n",
    "            ac += 1\n",
    "        if w == \"b\":\n",
    "            bc += 1\n",
    "    return np.asarray([ac,bc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ffs = [length, letter_in_word, letter_freq_in_word, number_of_a_and_b]\n",
    "dataset = [\"a\", \"b\", \"xx\", \"casa\", \"dbba\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_a_and_b running...\n",
      "number_of_a_and_b running...\n",
      "number_of_a_and_b running...\n",
      "number_of_a_and_b running...\n",
      "number_of_a_and_b running...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [0, 0],\n",
       "       [2, 0],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_a_and_b.matrix_of(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter_in_word running...\n",
      "letter_in_word running...\n",
      "letter_in_word running...\n",
      "letter_in_word running...\n",
      "letter_in_word running...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('a', 1.0)],\n",
       " [('b', 1.0)],\n",
       " [('x', 2.0)],\n",
       " [('a', 2.0), ('c', 1.0), ('s', 1.0)],\n",
       " [('a', 1.0), ('b', 2.0), ('d', 1.0)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_freq_in_word.matrix_of(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter_in_word running...\n",
      "letter_in_word running...\n",
      "letter_in_word running...\n",
      "letter_in_word running...\n",
      "letter_in_word running...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('a', 1.0)],\n",
       " [('b', 1.0)],\n",
       " [('x', 1.0), ('x', 1.0)],\n",
       " [('c', 1.0), ('a', 1.0), ('s', 1.0), ('a', 1.0)],\n",
       " [('d', 1.0), ('b', 1.0), ('b', 1.0), ('a', 1.0)]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_in_word.matrix_of(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dataless_finer]",
   "language": "python",
   "name": "conda-env-dataless_finer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
