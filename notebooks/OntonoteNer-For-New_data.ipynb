{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sklearn vsersion -> 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# core libs\n",
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "import json\n",
    "import yaml\n",
    "import codecs\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import cPickle as pickle\n",
    "\n",
    "from functools import wraps\n",
    "from unidecode import unidecode\n",
    "\n",
    "# numerical and ml libs\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack, csr_matrix\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import sklearn\n",
    "print(\"using sklearn vsersion -> %s\" % sklearn.__version__)\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn import linear_model, datasets, svm, ensemble\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# project files\n",
    "from dfiner.ontonote.ontonotes_data import load_ontonotes, read_figer, GoldMentionView\n",
    "\n",
    "from dfiner.utils import get_default_config, get_size, dump_pickle, load_pickle\n",
    "from dfiner.datastructures.utils import print_view\n",
    "from dfiner.annotators.hyp_pattern_annotator import HypPatternAnnotator\n",
    "from dfiner.annotators.kb_bias_annotator import KBBiasTypeAnnotator\n",
    "from dfiner.annotators.nsd_annotator import NSDView\n",
    "from dfiner.annotators.fine_type_annotator import FineTypeView, SynsetFineTyper\n",
    "from dfiner.annotators import get_non_default_annotator\n",
    "import dfiner.ontonotes_annotation_extender as oae\n",
    "from dfiner.topics import get_embedding_func, EmbeddingsType\n",
    "from dfiner.classifier.lexicon import Lexicon\n",
    "from dfiner.classifier.feature_function import DenseFeatureFunction, FeatureExtractor, FeatureFunction, FeatureStroage\n",
    "import pandas as pd\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# import polylearn\n",
    "\n",
    "FeatureStroage.set_cache_dir(\"/home/haowu4/.py_cache\")\n",
    "config = get_default_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 136 ms, total: 2.68 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading resources.\n",
    "common_cities_names_csv = set(pd.read_csv(config[\"common_us_city_path\"], names=[\"Name\", \"State\", \"Pop\"])[\"Name\"].tolist())\n",
    "common_cities_names = {x.strip() for x in common_cities_names_csv}\n",
    "\n",
    "kba = KBBiasTypeAnnotator(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'education.department': 0.1188118811881188,\n",
       " u'organization.educational_institution': 0.019801980198019802,\n",
       " u'organization.sports_team': 0.8613861386138614}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kba.surface_to_type_dist[\"University of Washington\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 43193 train docs\n",
      "loaded 9518 test docs\n",
      "loaded 434 figer docs\n",
      "loaded 434 figer gold docs\n",
      "loaded 4061 figer gold docs\n",
      "CPU times: user 57.5 s, sys: 236 ms, total: 57.8 s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_docs = load_ontonotes(nlp, \"/home/haowu4/data/simple_finer/NotAnnotated.name\")\n",
    "train_docs = load_ontonotes(nlp, \"/home/haowu4/Downloads/non_nw_train_dev.name\")\n",
    "# _ = [annotator(doc) for doc in annotated_ontonote_docs for annotator in non_default_annotators[:2]]\n",
    "\n",
    "test_docs = load_ontonotes(nlp, config['ontonotes_test_path'])\n",
    "figer_docs = read_figer(nlp, config['figer_path'])\n",
    "figer_gold_docs = read_figer(nlp, config['figer_gold'])\n",
    "annotated_ontonote_docs = read_figer(nlp, \"/home/haowu4/codes/dataless_finer/data/big_one.label\")\n",
    "\n",
    "print \"loaded %d train docs\" % len(train_docs)\n",
    "print \"loaded %d test docs\" % len(test_docs)\n",
    "print \"loaded %d figer docs\" % len(figer_docs)\n",
    "print \"loaded %d figer gold docs\" % len(figer_gold_docs)\n",
    "print \"loaded %d figer gold docs\" % len(annotated_ontonote_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/organization,/news_agency:Xinhua News Agency\n",
      "/time:February 13\n",
      "/location/country,/location:Beijing\n"
     ]
    }
   ],
   "source": [
    "print_view(annotated_ontonote_docs[0], \"gold_mention_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def annotate_extend(docs):\n",
    "    _ = [oae.add_titles(doc, oae.title_set) for doc in docs]\n",
    "    _ = [oae.add_typexs('MEDICINE', doc, oae.symptom_alias_set.union(oae.drug_set.union(oae.treatment_set))) for doc in docs]\n",
    "    _ = [oae.add_typexs('ANIMAL', doc, oae.animal_set) for doc in docs]\n",
    "    _ = [oae.add_typexs('ROAD', doc, oae.road_set) for doc in docs]\n",
    "    _ = [oae.fix_type1_to_type2(doc, \"ORG\", \"FAC\", oae.facility_as_org_trigger_words) for doc in docs]\n",
    "    _ = [oae.fix_type1_to_type2(doc, \"FAC\", \"ROAD\", oae.road_set) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 180 ms, total: 29.5 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "annotate_extend(train_docs)\n",
    "annotate_extend(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyp_pattern_annotator = HypPatternAnnotator(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 1.78 s, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "non_default_annotators = get_non_default_annotator(nlp, config)\n",
    "# noun_and_rule_annotators = non_default_annotators[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dfiner.annotators.hyp_pattern_annotator.HypPatternAnnotator at 0x7f9c24429ad0>,\n",
       " <dfiner.annotators.nsd_annotator.NounSenseAnnotator at 0x7f9c24429b90>,\n",
       " <dfiner.annotators.fine_type_annotator.RuleBasedFineTypeAnnotator at 0x7f9c24429bd0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_default_annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 13s, sys: 424 ms, total: 2min 13s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%prun -s cumulative\n",
    "_ = [annotator(doc) for doc in train_docs for annotator in non_default_annotators[:2]]\n",
    "_ = [annotator(doc) for doc in test_docs for annotator in non_default_annotators[:2]]\n",
    "_ = [annotator(doc) for doc in figer_docs for annotator in non_default_annotators[:2]]\n",
    "_ = [annotator(doc) for doc in figer_gold_docs for annotator in non_default_annotators[:2]]\n",
    "_ = [annotator(doc) for doc in annotated_ontonote_docs for annotator in non_default_annotators[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# _ = [annotator(doc) for doc in annotated_ontonote_docs for annotator in non_default_annotators[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(config[\"ontonote_to_figer_map\"]) as f_in:\n",
    "    ontonotes_to_figer_course = yaml.load(f_in)\n",
    "\n",
    "def type_func(mention_constituent):\n",
    "    return ontonotes_to_figer_course[mention_constituent.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def slash_to_dot(slash_str):\n",
    "    return \".\".join(slash_str.split(\"/\")[1:])\n",
    "\n",
    "def get_figer_type_func(kba):\n",
    "    def figer_type_func(mention_constituent):\n",
    "        all_types = map(slash_to_dot, mention_constituent.name.split(\",\"))\n",
    "        try:\n",
    "            course_types = list(set(map(kba.type_system.get_root, all_types)))\n",
    "            if len(course_types) > 1:\n",
    "                print \"CAUTION: more than one course types -> (%s). Assigining (%s).\" % (course_types, course_types[0])\n",
    "                return course_types[0]\n",
    "            return course_types[0]\n",
    "        except:\n",
    "            print \"CAUTION: encoutered error while looking up (%s)\" % (all_types)\n",
    "            return None\n",
    "    return figer_type_func\n",
    "\n",
    "figer_type_func = get_figer_type_func(kba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 348 ms, total: 2.12 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we want full w2v\n",
    "w2v_500k_pickle_path = config[\"embeddings_cache_path\"]\n",
    "with open(w2v_500k_pickle_path) as f_in:\n",
    "    w2vdict = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "default_w2v_mean = np.mean(list(w2vdict.values()), axis=0)\n",
    "default_w2v_zero = np.zeros(default_w2v_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in extended_w2vdict = 804253\n"
     ]
    }
   ],
   "source": [
    "# extended w2v with lowercase letters\n",
    "extended_w2vdict = {}\n",
    "for w in w2vdict:\n",
    "    extended_w2vdict[w] = w2vdict[w]\n",
    "    if w.lower() not in w2vdict:\n",
    "        extended_w2vdict[w.lower()] = w2vdict[w]\n",
    "\n",
    "print \"# words in extended_w2vdict = %d\" % len(extended_w2vdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lsi_embedding_func = get_embedding_func(config, EmbeddingsType.LSI, 100)\n",
    "lda_embedding_func = get_embedding_func(config, EmbeddingsType.LDA, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "class GazetteerReader():\n",
    "    def __init__(self, base_folder, exclude = {}, exclude_begin = {}, exclude_end = {}):\n",
    "        self.base_folder = base_folder\n",
    "        gzs = defaultdict(set)\n",
    "        start_counter = defaultdict(list)\n",
    "        end_counter = defaultdict(list)\n",
    "        \n",
    "        for gz_entry in os.listdir(base_folder):\n",
    "            if gz_entry in exclude:\n",
    "                continue\n",
    "            with open(os.path.join(base_folder,gz_entry)) as inp:\n",
    "                for line in inp:\n",
    "                    line = line.strip()\n",
    "                    gzs[gz_entry].add(line)\n",
    "                    ws = line.split()\n",
    "                    if gz_entry not in exclude_begin:\n",
    "                        start_counter[ws[0]].append(gz_entry)\n",
    "                    if gz_entry not in exclude_end:\n",
    "                        end_counter[ws[-1]].append(gz_entry)\n",
    "\n",
    "        self.gzs = defaultdict(set)\n",
    "        \n",
    "        for gz_entry, gz_sets in gzs.iteritems():\n",
    "            \n",
    "            self.gzs[gz_entry] = gz_sets\n",
    "            continue\n",
    "            \n",
    "            none_uniq = set()\n",
    "            for gz_entry_2, gz_sets_2 in gzs.iteritems():\n",
    "                if gz_entry == gz_entry_2:\n",
    "                    continue\n",
    "                for x in gz_sets.intersection(gz_sets_2):\n",
    "                    none_uniq.add(x)\n",
    "                    self.gzs[\"%s-and-%s\" % (gz_entry,gz_entry_2)].add(x)\n",
    "                    \n",
    "            uniq_sets = gz_sets.difference(none_uniq)\n",
    "#             uniq_sets = gz_sets.difference(none_uniq)\n",
    "            self.gzs[gz_entry] = uniq_sets\n",
    "            \n",
    "        self.begins = {}\n",
    "        self.ends = {}\n",
    "        \n",
    "        for w, obz in start_counter.iteritems():            \n",
    "            target_, _ = Counter(obz).most_common(1)[0]\n",
    "            self.begins[w] = target_\n",
    "            \n",
    "        \n",
    "        for w, obz in end_counter.iteritems():            \n",
    "            target_, _ = Counter(obz).most_common(1)[0]\n",
    "            self.ends[w] = target_\n",
    "\n",
    "            \n",
    "    def found_exact_match(self, surface):\n",
    "        ret = [a for a,b in self.gzs.iteritems() if surface in b or surface.lower() in b]\n",
    "        return ret\n",
    "        \n",
    "    def begin_match(self, surface):\n",
    "        if surface not in self.begins:\n",
    "            return []\n",
    "        ret = self.begins[surface]\n",
    "        return [ret]\n",
    "    \n",
    "    def end_match(self, surface):\n",
    "        if surface not in self.ends:\n",
    "            return []\n",
    "\n",
    "        ret = self.ends[surface]\n",
    "        return [ret]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gaz_reader = GazetteerReader(\"/home/haowu4/data/gazetteers/\", \n",
    "                             exclude={\"all\", \"GPE.filter\"},\n",
    "                             exclude_begin={\"loc\", \"org\", \"ORG.filter\"},\n",
    "                             exclude_end={\"loc\", \"org\", \"ORG.filter\"}\n",
    "                            )\n",
    "# gaz_reader.gzs['org'].add(\"Washington State\")\n",
    "# gaz_reader.gzs['org'].add(\"San Antonio\")\n",
    "# gaz_reader.gzs['org'].add(\"Utah\")\n",
    "# gaz_reader.gzs['org'].add(\"Cal\")\n",
    "# gaz_reader.gzs['org'].add(\"Rampage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['per']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz_reader.end_match(\"Balch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_shape_func(text):\n",
    "    text = re.sub(\"[a-z]+\", \"a\" ,text)\n",
    "    text = re.sub(\"[A-Z]+\", \"A\" ,text)\n",
    "    text = re.sub(\"[0-9]+\", \"0\" ,text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def gazetteer_feature(gzr):\n",
    "    @FeatureFunction(\"gazetteer_feature\")\n",
    "    def f(doc, mention):\n",
    "        surface = doc[mention.start:mention.end].text.strip()\n",
    "#         print(surface)\n",
    "        matches = gzr.found_exact_match(surface)\n",
    "        return matches\n",
    "\n",
    "    f.__name__ = \"gazetteer_feature\"\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def gazetteer_begin(gzr):\n",
    "    @FeatureFunction(\"gazetteer_begin\")\n",
    "    def f(doc, mention):\n",
    "        surface = doc[mention.start].text.strip()\n",
    "#         print(surface)\n",
    "        matches = gzr.begin_match(surface)\n",
    "        return matches\n",
    "\n",
    "    f.__name__ = \"gazetteer_begin\"\n",
    "    \n",
    "    return f\n",
    "\n",
    "def gazetteer_end(gzr):\n",
    "    @FeatureFunction(\"gazetteer_end\")\n",
    "    def f(doc, mention):\n",
    "        surface = doc[mention.end-1].text.strip()\n",
    "#         print(surface)\n",
    "        matches = gzr.end_match(surface)\n",
    "        return matches\n",
    "\n",
    "    f.__name__ = \"gazetteer_end\"\n",
    "    \n",
    "    return f\n",
    "\n",
    "def common_cities_feature(cities_names_set):\n",
    "    @FeatureFunction(\"common_cities_feature\")\n",
    "    def f(doc, mention):\n",
    "        surface = doc[mention.start:mention.end].text.strip()\n",
    "#         print(surface)\n",
    "        if surface in cities_names_set:\n",
    "            return [\"1True\"]\n",
    "        else:\n",
    "            return [\"2False\"]\n",
    "\n",
    "    \n",
    "    f.__name__ = \"common_cities_fn\"\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def ngram_before(n):\n",
    "    @FeatureFunction(\"%dgram_before\" % n)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        if start - n >= 0:\n",
    "            return [doc[start-n:start].text]\n",
    "        else:\n",
    "            return []\n",
    "    f.__name__ = \"%dgram_before\" % n\n",
    "    return f\n",
    "\n",
    "\n",
    "def ngram_after(n):\n",
    "    @FeatureFunction(\"%dgram_after\" % n)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        if end + n <= len(doc):\n",
    "            return [doc[end:end+n].text]\n",
    "        else:\n",
    "            return []\n",
    "    f.__name__ = \"%dgram_after\" % n\n",
    "    return f\n",
    "\n",
    "\n",
    "@FeatureFunction(\"dep_feature\")\n",
    "def mention_details(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    heads = [token.head for token in doc[start:end]]\n",
    "    deps = [list(token.children) for token in doc[start:end]]\n",
    "    for token, head, children in zip(doc[start:end], heads, deps):\n",
    "        if not (head.i >= start and head.i < end):            \n",
    "            yield \"<-%s- %s\" % (token.dep_, head.lemma_)\n",
    "            yield \"<- %s\" % (head.lemma_)\n",
    "        for child in children:\n",
    "            if not (child.i >= start and child.i < end):\n",
    "                yield  \"-%s-> %s\" % (child.dep_,child.lemma_)\n",
    "                yield  \"-> %s\" % (child.lemma_)\n",
    "\n",
    "\n",
    "@FeatureFunction(\"dep_feature_len2\")\n",
    "def dep_feat_len2(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    heads = [token.head for token in doc[start:end]]\n",
    "    deps = [list(token.children) for token in doc[start:end]]\n",
    "    for token, head, children in zip(doc[start:end], heads, deps):\n",
    "        # len 2 deps through head\n",
    "        if head.head != head and not (start <= head.head.i < end):\n",
    "            yield \"<-%s- %s <-%s- %s\" % (token.dep_, head.lemma_, head.dep_, head.head.lemma_)\n",
    "            yield \"<- %s <- %s\" % (head.lemma_, head.head.lemma_)\n",
    "        for child in head.children:\n",
    "            if child == token or start <= child.i < end:\n",
    "                continue\n",
    "            yield \"<-%s- %s -%s-> %s\" % (token.dep_, head.lemma_, child.dep_, child.lemma_)\n",
    "            yield \"<- %s -> %s\" % (head.lemma_, child.lemma_)\n",
    "        # len 2 deps through children\n",
    "        for child in children:\n",
    "            for grandchild in child.children:\n",
    "                if start <= grandchild.i < end:\n",
    "                    continue\n",
    "                yield  \"-%s-> %s -%s-> %s\" % (child.dep_, child.lemma_, grandchild.dep_, grandchild.lemma_)\n",
    "                yield  \"-> %s -> %s\" % (child.lemma_, grandchild.lemma_)\n",
    "\n",
    "\n",
    "def word_before(position):\n",
    "    @FeatureFunction(\"word_before_%d\" % position)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        for i in range(max(start-position,0), start):\n",
    "            yield doc[i].text\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_before_loc(position):\n",
    "    @FeatureFunction(\"word_before_loc_%d\" % position)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        for i in range(max(start-position,0), start):\n",
    "            yield \"%d-%s\" % (start - i,doc[i].text)\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_before_lemma(position):\n",
    "    @FeatureFunction(\"word_before_lemma_%d\" % position)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        for i in range(max(start-position,0), start):\n",
    "            yield doc[i].lemma_\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_after(position):\n",
    "    @FeatureFunction(\"word_after_%d\" % position)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        for i in range(end, min(end+position,len(doc))):\n",
    "            yield doc[i].text\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_after_loc(position):\n",
    "    @FeatureFunction(\"word_after_loc_%d\" % position)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        for i in range(end, min(end+position,len(doc))):\n",
    "            yield \"%d-%s\" % (i - end,doc[i].text)\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_after_lemma(position):\n",
    "    @FeatureFunction(\"word_after_lemma_%d\" % position)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        for i in range(end, min(end+position,len(doc))):\n",
    "            yield doc[i].lemma_\n",
    "        \n",
    "    return f\n",
    "\n",
    "\n",
    "@FeatureFunction(\"wim_shape\")\n",
    "def word_shape_in_mention(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for token in doc[start:end]:\n",
    "        yield word_shape_func(token.text)\n",
    "\n",
    "\n",
    "@FeatureFunction(\"wim\")\n",
    "def word_in_mention(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for token in doc[start:end]:\n",
    "        yield token.text\n",
    "\n",
    "\n",
    "@FeatureFunction(\"wim_lemma\")\n",
    "def word_in_mention_lemma(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for token in doc[start:end]:\n",
    "        yield token.lemma_\n",
    "        \n",
    "\n",
    "@FeatureFunction(\"wim_loc\")\n",
    "def word_in_mention_loc(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for i,x in enumerate(doc[start:end]):\n",
    "        yield \"f%d-%s\" % (i,x.text)\n",
    "        yield \"b%d-%s\" % ((end-start-1) - i,x.text)\n",
    "        \n",
    "\n",
    "@FeatureFunction(\"wim_loc_lemma\")\n",
    "def word_in_mention_loc_lemma(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for i,x in enumerate(doc[start:end]):\n",
    "        x = x.lemma_\n",
    "        yield \"f%d-%s\" % (i,x)\n",
    "        yield \"b%d-%s\" % ((end-start-1) - i,x)\n",
    "    \n",
    "\n",
    "def wim_ngram(n=2):\n",
    "    @FeatureFunction(\"wim_%dgram\" % n)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        words = map(lambda token: token.text, doc[start:end])\n",
    "        for ngram_tup in zip(*[words[i:] for i in xrange(n)]):\n",
    "            yield \"-\".join(ngram_tup)\n",
    "    return f\n",
    "        \n",
    "\n",
    "def wim_ngram_lemma(n=2):\n",
    "    @FeatureFunction(\"wim_%dgram_lemma\" % n)\n",
    "    def f(doc, mention):\n",
    "        start, end = mention.start, mention.end\n",
    "        words = map(lambda token: token.lemma_, doc[start:end])\n",
    "        for ngram_tup in zip(*[words[i:] for i in xrange(n)]):\n",
    "            yield \"-\".join(ngram_tup)\n",
    "    return f\n",
    "\n",
    "\n",
    "@FeatureFunction(\"mention_shape\")\n",
    "def mention_shape(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    t = \" \".join([x.text for x in doc[start:end]])\n",
    "    return [word_shape_func(t)]\n",
    "        \n",
    "\n",
    "@FeatureFunction(\"mention_length\")\n",
    "def mention_length(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    return [\"%d\" % (end-start)]\n",
    "        \n",
    "\n",
    "@FeatureFunction(\"is_all_cap\")\n",
    "def is_all_cap(doc, mention):\n",
    "    surface = doc[mention.start:mention.end].text.strip()\n",
    "    for c in surface:\n",
    "        if c.isalpha() and c.islower():\n",
    "            return [\"False\"]\n",
    "    return [\"True\"]\n",
    "\n",
    "\n",
    "@FeatureFunction(\"has_non_alphanum\")\n",
    "def has_non_alphanum(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    ret = set()\n",
    "    surface = doc[mention.start:mention.end].text.strip()\n",
    "    for w in doc[start:end]:\n",
    "        for c in w.text:\n",
    "            if not c.isalnum():\n",
    "                ret.add(\"w=%s\" % w) \n",
    "                ret.add( \"c=%s\" % c) \n",
    "#                 break\n",
    "    return list(ret)\n",
    "\n",
    "\n",
    "@FeatureFunction(\"one_word_endding\")\n",
    "def one_word_endding(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    if start - end == 1:\n",
    "        return [doc[end].text[-2:]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    \n",
    "\n",
    "@FeatureFunction(\"one_word_ge2_cap\")\n",
    "def one_word_ge2_cap(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    if start - end == 1:\n",
    "        w = doc[start].text\n",
    "        if len([c for c in w if c.isupper()]) >= 2:\n",
    "            return [\"True\"]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "#     for c in surface:\n",
    "#         if not c.isalnum():\n",
    "#             yield c\n",
    "            \n",
    "#     for i in range(max(start-position,0), start):\n",
    "#         yield \"%d-%s\" % (start - i,doc[i].text)\n",
    "\n",
    "#     return [\"True\"]\n",
    "\n",
    "@FeatureFunction(\"prefix\")\n",
    "def prefix(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for w in doc[start:end]:\n",
    "        for i in range(3, min(5, len(w.text))):\n",
    "            yield w.text[:i]\n",
    "        \n",
    "@FeatureFunction(\"length_ge\")\n",
    "def length_ge(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for i in range(1,6):\n",
    "        if end - start >= i:\n",
    "            yield \"ge_%d\" % i\n",
    "        else:\n",
    "            yield \"le_%d\" % i\n",
    "            \n",
    "@FeatureFunction(\"suffix\")\n",
    "def postfix(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    for w in doc[start:end]:\n",
    "        for i in range(3, min(5, len(w.text))):\n",
    "            yield w.text[-i:]\n",
    "\n",
    "\n",
    "# KB-Bias features\n",
    "def kbbias(kbbias_annotator):\n",
    "    @FeatureFunction(\"kbbias-new\")\n",
    "    def wrappee(doc, mention):\n",
    "        surface = doc[mention.start:mention.end].text\n",
    "        results = None\n",
    "        if surface in kbbias_annotator.surface_to_type_dist:\n",
    "            results = kbbias_annotator.surface_to_type_dist[surface]\n",
    "        elif (surface[:4].lower() == 'the ') and \\\n",
    "              surface[4:] in kbbias_annotator.surface_to_type_dist:\n",
    "            results = kbbias_annotator.surface_to_type_dist[surface[4:]]\n",
    "        if results:\n",
    "            return results.iteritems()\n",
    "        else:\n",
    "            return []\n",
    "    return wrappee\n",
    "\n",
    "\n",
    "def get_most_sim_from_gensim(gensim_w2v, query, topn):\n",
    "    if query in gensim_w2v.cached_most_sim:\n",
    "        return gensim_w2v.cached_most_sim[query]\n",
    "    else:\n",
    "        r = list(gensim_w2v.most_similar(query, topn=topn))\n",
    "        gensim_w2v.cached_most_sim[query] = r\n",
    "        return r\n",
    "\n",
    "# KB-Bias features\n",
    "def kbbias_approx(kbbias_annotator, gensim_word2vec, min_sim=0.5):\n",
    "    @FeatureFunction(\"kbbias_approx\")\n",
    "    def wrappee(doc, mention):\n",
    "        if mention.end - mention.start != 1:\n",
    "            return []\n",
    "        surface = doc[mention.start:mention.end].text\n",
    "        results_dist = defaultdict(float)\n",
    "        total_count = 0.0\n",
    "        if surface not in gensim_word2vec.vocab:\n",
    "            return []\n",
    "        sims = get_most_sim_from_gensim(gensim_word2vec, surface, 4)\n",
    "        for approx_word, sim_score in sims:\n",
    "            results = None\n",
    "            if sim_score < sim_score:\n",
    "                continue\n",
    "            if approx_word in kbbias_annotator.surface_to_type_dist:\n",
    "                results = kbbias_annotator.surface_to_type_dist[approx_word]\n",
    "            elif (approx_word[:4].lower() == 'the ') and \\\n",
    "                  approx_word[4:] in kbbias_annotator.surface_to_type_dist:\n",
    "                results = kbbias_annotator.surface_to_type_dist[approx_word[4:]]\n",
    "            if results:\n",
    "                for k in results:\n",
    "                    results_dist[k] += results[k]                   \n",
    "                total_count += 1.0\n",
    "        if total_count > 0:\n",
    "            for k in results_dist:\n",
    "                results_dist[k] = results_dist[k]   / total_count                 \n",
    "            return results_dist.iteritems()\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return wrappee\n",
    "\n",
    "@FeatureFunction('has_number_inside')\n",
    "def has_number_inside(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    ret = set()\n",
    "    surface = doc[mention.start:mention.end].text.strip()\n",
    "    for c in surface:\n",
    "        if c.isdigit():\n",
    "            return [\"True\"]\n",
    "    return [\"False\"]\n",
    "#     for w in doc[start:end]:\n",
    "#         for c in w.text:\n",
    "#             if not c.isalnum():\n",
    "#                 ret.add(\"w=%s\" % w) \n",
    "#                 ret.add( \"c=%s\" % c) \n",
    "# #                 break\n",
    "#     return list(ret)\n",
    "\n",
    "\n",
    "@FeatureFunction(\"in_quotes\")\n",
    "def in_quotes(doc, mention):\n",
    "    \n",
    "    start, end = mention.start, mention.end\n",
    "    \n",
    "    if any([unidecode(doc[i].text) == '\"' for i in xrange(max(0, start-2), start+1)]) and \\\n",
    "       any([unidecode(doc[i].text) == '\"' for i in xrange(end-1, min(end+2, len(doc)))]):\n",
    "            return [\"in-double-quotes\"]\n",
    "    \n",
    "    if any([unidecode(doc[i].text) == \"'\" for i in xrange(max(0, start-2), start+1)]) and \\\n",
    "       any([unidecode(doc[i].text) == \"'\" for i in xrange(end-1, min(end+2, len(doc)))]):\n",
    "            return [\"in-single-quotes\"]\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "synset_typer = SynsetFineTyper(config)\n",
    "take_best_sense = True\n",
    "\n",
    "\n",
    "@FeatureFunction(\"hyp_fine_types\")\n",
    "def hyp_fine_type_feats(doc, mention):\n",
    "    hyp_view = doc.user_data[HypPatternAnnotator.HYP_VIEW]\n",
    "    nsd_view = doc.user_data[NSDView.NSD_VIEW_NAME]\n",
    "\n",
    "    fine_type_scores = defaultdict(float)\n",
    "    hyp_pattern_fine_type_scores = defaultdict(float)\n",
    "    for token_constituent in hyp_view.constituents:\n",
    "        # mention covers this token\n",
    "        if mention.start <= token_constituent.start < mention.end:\n",
    "            if token_constituent.incoming_relations is None:\n",
    "                continue\n",
    "            for relation in token_constituent.incoming_relations:\n",
    "                # checking if source is inside the mention. could remove it too\n",
    "                hypernym_token_constituent = relation.source\n",
    "                if mention.start <= hypernym_token_constituent.start < mention.end:\n",
    "                    continue\n",
    "                # print doc[hypernym_token_constituent.start:hypernym_token_constituent.end]\n",
    "                for nsd_constituent in nsd_view.constituents:\n",
    "                    if nsd_constituent.start <= hypernym_token_constituent.start < nsd_constituent.end:\n",
    "                        # print nsd_constituent.label2score\n",
    "                        if take_best_sense:\n",
    "                            synset_offset_pos, score = max(nsd_constituent.label2score.items(), key=itemgetter(1))\n",
    "                            for fine_type in synset_typer.get_fine_types(synset_offset_pos):\n",
    "                                fine_type_scores[fine_type] += 1.\n",
    "                                hyp_pattern_fine_type_scores[relation.relation_name +\"=>\" + fine_type] += 1.\n",
    "                        else:\n",
    "                            for synset_offset_pos, score in nsd_constituent.label2score.iteritems():\n",
    "                                for fine_type in synset_typer.get_fine_types(synset_offset_pos):\n",
    "                                    # print fine_type\n",
    "                                    fine_type_scores[fine_type] += score\n",
    "                                    hyp_pattern_fine_type_scores[relation.relation_name +\"=>\" + fine_type] += score\n",
    "    return fine_type_scores.items() + hyp_pattern_fine_type_scores.items()\n",
    "\n",
    "\n",
    "@FeatureFunction(\"bias\")\n",
    "def CONSTANT_BIAS(doc, mention):\n",
    "    start, end = mention.start, mention.end\n",
    "    return [\"bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getOrDefault(m, k, d):\n",
    "    if k in m:\n",
    "        return m[k]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "\n",
    "def topicSentence(sentEmbeddingFunc, feature_name, size):\n",
    "    @DenseFeatureFunction(size)\n",
    "    def topicSentence(doc, mention):\n",
    "        tokenized_doc = [token.text for token in doc]\n",
    "        return sentEmbeddingFunc(tokenized_doc)\n",
    "    topicSentence.__name__ = feature_name\n",
    "    return topicSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ngrams_matches(token_sequence, ngram_set, max_n):\n",
    "    \"\"\"\n",
    "    From the token_sequence return all ngrams of size <= max_n.\n",
    "    Any ngram should be from the given ngram_set.\n",
    "    Ensure all tokens in the token_sequence is part of one and exactly one ngram.\n",
    "    \"\"\"\n",
    "    non_overlapping_matches = []\n",
    "    l = len(token_sequence)\n",
    "    max_n = min(l, max_n)\n",
    "    for n in xrange(max_n, 0, -1):\n",
    "        i = 0\n",
    "        while i <= l-n:\n",
    "            if any([(match[1][0]<=i<match[1][1] or match[1][0]<=i+n<match[1][1]) for match in non_overlapping_matches]):\n",
    "                # overlapping with a higher ngram. skip.\n",
    "                i += n\n",
    "                continue\n",
    "            s = \"_\".join(token_sequence[i:i+n])\n",
    "            if s in ngram_set:\n",
    "                non_overlapping_matches.append((s, (i, i+n)))\n",
    "                i += n\n",
    "                continue\n",
    "            s_lower = s.lower()\n",
    "            if s_lower in ngram_set:\n",
    "                non_overlapping_matches.append((s_lower, (i, i+n)))\n",
    "                i += n\n",
    "                continue\n",
    "            i += 1\n",
    "    return non_overlapping_matches\n",
    "                \n",
    "\n",
    "def w2vMention(w2v_dict, default_w2v, max_n=3):\n",
    "    @DenseFeatureFunction(300)\n",
    "    def w2vMention(doc, mention):\n",
    "        l = mention.end - mention.start\n",
    "        if l == 0:\n",
    "            print(doc)\n",
    "            print(\"WARNING: The length of the mention is 0!\")\n",
    "        mention_tokens = [token.text for token in doc[mention.start:mention.end]]\n",
    "        mean_vecs = [w2v_dict[match[0]] for match in get_ngrams_matches(mention_tokens, w2v_dict, max_n)]\n",
    "        return np.mean(mean_vecs, axis=0) if len(mean_vecs) else default_w2v\n",
    "    return w2vMention\n",
    "\n",
    "def w2vMentionExactMatch(w2v_dict, default_w2v, max_n=3):\n",
    "    @DenseFeatureFunction(300)\n",
    "    def w2vMentionExactMatch(doc, mention):\n",
    "        surface = doc[mention.start:mention.end].text.replace(\" \",\"_\")\n",
    "        ret = w2v_dict[surface] if surface in w2v_dict else default_w2v\n",
    "        return ret\n",
    "    return w2vMentionExactMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text = \"The “ working ” German shepherds and Belgian malinois \" + \\\n",
    "                 \"he cared for were either attack dogs or bomb or narcotics sniffers \"+ \\\n",
    "                 \", he said , standing in his year-old Baxter Creek Veterinary Clinic . \"\n",
    "token_sequence = text.decode('utf-8').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'German_shepherds', (4, 6)),\n",
       " (u'belgian_malinois', (7, 9)),\n",
       " (u'he_said', (22, 24)),\n",
       " (u'in_his', (26, 28)),\n",
       " (u'Veterinary_Clinic', (31, 33)),\n",
       " (u'The', (0, 1)),\n",
       " (u'working', (2, 3)),\n",
       " (u'he', (9, 10)),\n",
       " (u'cared', (10, 11)),\n",
       " (u'for', (11, 12)),\n",
       " (u'were', (12, 13)),\n",
       " (u'either', (13, 14)),\n",
       " (u'attack', (14, 15)),\n",
       " (u'dogs', (15, 16)),\n",
       " (u'or', (16, 17)),\n",
       " (u'bomb', (17, 18)),\n",
       " (u'or', (18, 19)),\n",
       " (u'narcotics', (19, 20)),\n",
       " (u'sniffers', (20, 21)),\n",
       " (u'Baxter', (29, 30))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ngrams_matches(token_sequence, extended_w2vdict, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AAAS is the largest scientific society in the world and publishes journals such as Science , and Science Translational Medicine . \n",
      "\n",
      "journals -> [ hearst1 -> Science, hearst1 -> Medicine, hearst1 -> Science ]\n",
      "Science\n",
      "AAAS -> [ hearst_rev_copular -> society ]\n",
      "Science\n",
      "Medicine\n",
      "Translational -> [ hearst_ncompmod -> Science, hearst_ncompmod -> Medicine, hearst_ncompmod -> Science ]\n",
      "society -> [ hearst_copular -> AAAS ]\n"
     ]
    }
   ],
   "source": [
    "doc = figer_gold_docs[1]\n",
    "print doc\n",
    "print \"\"\n",
    "hyp_view = doc.user_data[HypPatternAnnotator.HYP_VIEW]\n",
    "nsd_view = doc.user_data[NSDView.NSD_VIEW_NAME]\n",
    "print_view(doc, HypPatternAnnotator.HYP_VIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'type_lex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cabd974f2e78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype_lex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexeme_to_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'type_lex' is not defined"
     ]
    }
   ],
   "source": [
    "type_lex.lexeme_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> (['building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> (['building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> (['building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', 'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization']). Assigining (building).\n",
      "CAUTION: more than one course types -> (['organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', 'organization']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> (['organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', 'work']). Assigining (organization).\n",
      "CAUTION: more than one course types -> (['norpl', u'location']). Assigining (norpl).\n",
      "CAUTION: more than one course types -> (['norpl', u'location']). Assigining (norpl).\n",
      "CAUTION: more than one course types -> (['norpl', u'location']). Assigining (norpl).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: encoutered error while looking up ([u'internet.website', u'organization.company', u'organization'])\n",
      "CAUTION: encoutered error while looking up ([u'internet.website'])\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> (['building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> (['building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> (['building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', 'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization']). Assigining (building).\n",
      "CAUTION: more than one course types -> (['organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', 'organization']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> (['organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'organization', u'location']). Assigining (organization).\n",
      "CAUTION: more than one course types -> ([u'building', u'organization', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'organization', 'work']). Assigining (organization).\n",
      "CAUTION: more than one course types -> (['norpl', u'location']). Assigining (norpl).\n",
      "CAUTION: more than one course types -> (['norpl', u'location']). Assigining (norpl).\n",
      "CAUTION: more than one course types -> (['norpl', u'location']). Assigining (norpl).\n",
      "CAUTION: more than one course types -> ([u'building', u'location']). Assigining (building).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: encoutered error while looking up ([u'internet.website', u'organization.company', u'organization'])\n",
      "CAUTION: encoutered error while looking up ([u'internet.website', u'organization'])\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n",
      "CAUTION: more than one course types -> ([u'transportation.road', u'location']). Assigining (transportation.road).\n"
     ]
    }
   ],
   "source": [
    "def build_corpora(docs, type_func, skip_none=True):\n",
    "    ret = []\n",
    "    for doc in docs:\n",
    "        for m in doc.user_data[\"gold_mention_view\"]:\n",
    "            if skip_none and type_func(m) is None:\n",
    "                continue\n",
    "            ret.append((doc, m))\n",
    "    return ret\n",
    "\n",
    "def build_ys_from_gold(corpora, type_lex, type_func):\n",
    "    ret = []\n",
    "    missing = set()\n",
    "    for doc, m in corpora:\n",
    "        t = type_func(m)\n",
    "        if t is None:\n",
    "            raise ValueError(\"type is None\")\n",
    "        type_lex.see_lexeme(t)\n",
    "        idx = type_lex.getOrNegOne(t)\n",
    "        if idx == -1:\n",
    "            missing.add(t)\n",
    "        ret.append(idx)\n",
    "    for t in missing:\n",
    "        print(\"missing label %s\" % t)\n",
    "    return np.array(ret)\n",
    "\n",
    "type_lex = Lexicon()\n",
    "    \n",
    "ontonote_train = build_corpora(train_docs, type_func)\n",
    "Y_train = build_ys_from_gold(ontonote_train, type_lex, type_func)\n",
    "\n",
    "ontonote_test = build_corpora(test_docs, type_func)\n",
    "Y_test = build_ys_from_gold(ontonote_test, type_lex, type_func)\n",
    "\n",
    "figer_test = build_corpora(figer_docs, figer_type_func)\n",
    "Y_figer = build_ys_from_gold(figer_test, type_lex, figer_type_func)\n",
    "\n",
    "figer_gold_test = build_corpora(figer_gold_docs, figer_type_func)\n",
    "Y_figer_gold = build_ys_from_gold(figer_gold_test, type_lex, figer_type_func)\n",
    "\n",
    "new_doc_test = build_corpora(annotated_ontonote_docs, figer_type_func)\n",
    "Y_new_doc = build_ys_from_gold(new_doc_test, type_lex, figer_type_func)\n",
    "\n",
    "\n",
    "# annotated_ontonote_docs\n",
    "# annotated_ontonote_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# features\n",
    "\n",
    "# sparse feature functions\n",
    "\n",
    "sparse_feat_funcs = [\n",
    "#     CONSTANT_BIAS,\n",
    "#     word_shape_in_mention,\n",
    "#     word_in_mention, word_in_mention_lemma,\n",
    "    is_all_cap, \n",
    "    has_number_inside,\n",
    "#     length_ge,\n",
    "#     has_non_alphanum,\n",
    "#     one_word_endding,\n",
    "    mention_details,\n",
    "    word_in_mention_loc,\n",
    "    word_in_mention_loc_lemma,\n",
    "    wim_ngram(2), wim_ngram_lemma(2),\n",
    "    wim_ngram(3), wim_ngram(3),\n",
    "    kbbias(kba), \n",
    "#     one_word_ge2_cap,\n",
    "#     common_cities_feature(common_cities_names),\n",
    "    gazetteer_feature(gaz_reader),\n",
    "    gazetteer_begin(gaz_reader),\n",
    "    gazetteer_end(gaz_reader),\n",
    "#     kbbias_approx(kba, gensim_word_vectors),\n",
    "    \n",
    "    hyp_fine_type_feats,\n",
    "    in_quotes,\n",
    "    ngram_before(3), #ngram_before(2),\n",
    "    ngram_after(3), #ngram_after(2)\n",
    "    dep_feat_len2,\n",
    "    w2vMentionExactMatch(extended_w2vdict, default_w2v_zero, max_n=3),\n",
    "#         w2vMention(extended_w2vdict, default_w2v_mean, max_n=3),\n",
    "#     topicSentence(lda_embedding_func,\"lda_embedding\",50),\n",
    "    topicSentence(lsi_embedding_func,\"lsi_embedding\",100),\n",
    "                    ]\n",
    "#             wim_ext, wim_ext_lemma,   \n",
    "#             word_shape,\n",
    "#             mention_length,\n",
    "#             prefix,\n",
    "#             postfix,\n",
    "#             mention_pronoun_wh_dep\n",
    "sparse_extractor= FeatureExtractor(sparse_feat_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CPU times: user 55.2 s, sys: 812 ms, total: 56 s\n",
      "Wall time: 55.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%prun -s cumulative\n",
    "\n",
    "force_update = True\n",
    "min_support = 3\n",
    "sparse_extractor.build_lexicon(ontonote_train, min_support=min_support, force_update=force_update)\n",
    "X_train = sparse_extractor.extract(\"ontonote-notannotated\", ontonote_train, force_update=force_update)\n",
    "X_test = sparse_extractor.extract(\"ontonote-test\", ontonote_test, force_update=force_update)\n",
    "X_figer = sparse_extractor.extract(\"figer-test\", figer_test, force_update=force_update)\n",
    "X_figer_gold = sparse_extractor.extract(\"figer-gold-test\", figer_gold_test, force_update=force_update)\n",
    "\n",
    "X_new_doc = sparse_extractor.extract(\"new-doc\", new_doc_test, force_update=force_update)\n",
    "\n",
    "feature_names = sparse_extractor.reverse_lexicon()\n",
    "\n",
    "index_to_feat = {i:f for i, f in enumerate(feature_names)}\n",
    "index_to_type = {i:t for t, i in type_lex.lexeme_to_index.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'3gram',\n",
       " u'dep_f',\n",
       " 'gazet',\n",
       " 'has_n',\n",
       " 'hyp_f',\n",
       " 'in_qu',\n",
       " 'is_al',\n",
       " u'kbbia',\n",
       " 'topic',\n",
       " 'w2vMe',\n",
       " u'wim_2',\n",
       " u'wim_3',\n",
       " u'wim_l'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{a[:5] for a in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# list(kbbias_approx(kba, gensim_word_vectors)(figer_docs[0], figer_docs[0].user_data[\"gold_mention_view\"].constituents[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figer_docs[0].user_data[\"gold_mention_view\"].constituents[1].start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50693, 39799)\n",
      "(15554, 39799)\n",
      "(561, 39799)\n",
      "(577, 39799)\n",
      "(7587, 39799)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print X_figer.shape\n",
    "print X_figer_gold.shape\n",
    "print X_new_doc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Eval Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metric_names = [\"micro-f1\", \"macro-f1\", \"weighted-f1\"]\n",
    "averages = [\"micro\", \"macro\", \"weighted\"]\n",
    "\n",
    "def print_global_metrics_header(metric_names):\n",
    "    print(\"%-15s %10s %10s %10s\\n\" % tuple([\"dataset\"] + metric_names))\n",
    "    \n",
    "def print_global_metrics(dataset_name, ys_gold, ys_pred, classes, averages):\n",
    "    print(\"%-15s %9.2f%% %9.2f%% %9.2f%%\" % tuple([dataset_name] + \n",
    "                                             [100*f1_score(ys_gold, ys_pred, classes, average=avg)\n",
    "                                              for avg in averages]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 124 ms, total: 11.6 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "def get_model_of(model_name):\n",
    "    if model_name == 0:\n",
    "        logreg = linear_model.SGDClassifier(\n",
    "                            loss='hinge',#'modified_huber',#'hinge',#perceptron',\n",
    "#                             penalty=\"l2\",\n",
    "#                             alpha=1e-6,\n",
    "#                             average=True,\n",
    "#                             class_weight='balanced'\n",
    "                            )\n",
    "#         sklearn.ensemble.BaggingClassifier(base_estimator=logreg, n_estimators=20)\n",
    "        return logreg\n",
    "\n",
    "    if model_name == 1:\n",
    "        logreg = linear_model.SGDClassifier(\n",
    "                            loss='hinge',#'modified_huber',#'hinge',#perceptron',\n",
    "#                             penalty=\"l2\",\n",
    "#                             alpha=1e-6,\n",
    "                            average=True,\n",
    "#                             class_weight='balanced'\n",
    "                            )\n",
    "        logreg = sklearn.ensemble.BaggingClassifier(base_estimator=logreg, n_estimators=20)\n",
    "        return logreg\n",
    "    \n",
    "logreg = get_model_of(0)\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset           micro-f1   macro-f1 weighted-f1\n",
      "\n",
      "train               98.01%     85.60%     98.00%\n",
      "test                91.22%     70.83%     91.01%\n",
      "figer               86.63%     41.90%     85.21%\n",
      "figer-gold          89.60%     51.29%     88.27%\n",
      "\n",
      "\n",
      "                  TYPE : train_f1  test_f1 figer_f1\n",
      "\n",
      "                 title :    0.997    0.994    0.917\n",
      "      finance.currency :    0.993    0.980    0.000\n",
      "                  time :    0.993    0.980    0.929\n",
      "          living_thing :    0.980    0.955    0.500\n",
      "              location :    0.983    0.944    0.878\n",
      "                 norpl :    0.981    0.943    0.909\n",
      "              medicine :    0.999    0.896    0.667\n",
      "                person :    0.984    0.889    0.962\n",
      "          organization :    0.967    0.865    0.891\n",
      "   transportation.road :    0.988    0.741    0.000\n",
      "                   law :    0.957    0.606    0.667\n",
      "               product :    0.935    0.586    0.000\n",
      "                 event :    0.961    0.577    0.182\n",
      "              building :    0.898    0.551    0.692\n",
      "                  work :    0.937    0.535    0.526\n",
      "                 award :    0.000    0.000    0.000\n",
      "              software :    0.000    0.000    0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haowu4/anaconda2/envs/finer/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/haowu4/anaconda2/envs/finer/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = logreg.predict(X_train)\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "y_figer_pred = logreg.predict(X_figer)\n",
    "y_figer_gold_pred = logreg.predict(X_figer_gold)\n",
    "y_new_doc_pred = logreg.predict(X_new_doc)\n",
    "classes = sorted(index_to_type.keys())\n",
    "\n",
    "averages = [\"micro\", \"macro\", \"weighted\"]\n",
    "print_global_metrics_header(metric_names)\n",
    "print_global_metrics(\"train\", Y_train, y_train_pred, classes, averages)\n",
    "print_global_metrics(\"test\", Y_test, y_test_pred, classes, averages)\n",
    "print_global_metrics(\"figer\", Y_figer, y_figer_pred, classes, averages)\n",
    "print_global_metrics(\"figer-gold\", Y_figer_gold, y_figer_gold_pred, classes, averages)\n",
    "print(\"\\n\")\n",
    "\n",
    "train_f1 = f1_score(Y_train, y_train_pred, classes, average=None).tolist()\n",
    "test_f1 = f1_score(Y_test, y_test_pred, classes, average=None).tolist()\n",
    "figer_gold_f1 = f1_score(Y_figer_gold, y_figer_gold_pred, classes, average=None).tolist()\n",
    "\n",
    "class_names = [None] * len(index_to_type)\n",
    "for i in range(len(index_to_type)):\n",
    "    class_names[i] = index_to_type[i]\n",
    "\n",
    "print(\"%22s : %8s %8s %8s\\n\" % (\"TYPE\", \"train_f1\", \"test_f1\", \"figer_f1\"))\n",
    "for type_name, train_type_f1, test_type_f1, figer_gold_type_f1 in \\\n",
    "    sorted([(index_to_type[index], train_f1[index], test_f1[index], figer_gold_f1[index])\n",
    "            for index in index_to_type], key=lambda a:a[2], reverse=True):\n",
    "    print(\"%22s : %8.3f %8.3f %8.3f\" % (type_name, train_type_f1, test_type_f1, figer_gold_type_f1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating Coarse Grain View, and annotating with fine-grain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from dfiner.datastructures import View, Constituent\n",
    "def add_pred_view(y_pred, docs_and_mentions, pred_viewname):\n",
    "    for doc, _ in docs_and_mentions:\n",
    "        if \"OntonoteType\" in doc.user_data:\n",
    "            del doc.user_data['OntonoteType']\n",
    "\n",
    "    assert len(y_pred) == len(docs_and_mentions)\n",
    "    for pred, (doc, mention) in zip(y_pred, docs_and_mentions):\n",
    "        user_data = doc.user_data\n",
    "        if pred_viewname not in user_data:\n",
    "            user_data[pred_viewname] = View()\n",
    "        view = user_data[pred_viewname]\n",
    "        label_name = type_lex.reverse_lex()[pred]\n",
    "        c = Constituent(mention.start,\n",
    "                        mention.end,\n",
    "                        name=pred_viewname,\n",
    "                        label2score={label_name: 1.0})\n",
    "        view.add_constituent(c)\n",
    "VIEW_NAME = \"OntonoteType\"\n",
    "add_pred_view(y_figer_pred, figer_test, VIEW_NAME)\n",
    "add_pred_view(y_figer_gold_pred, figer_gold_test, VIEW_NAME)\n",
    "add_pred_view(y_new_doc_pred, new_doc_test, VIEW_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "# with open(\"/home/haowu4/data/simple_finer/GoogleNews-vectors-negative300.combined_500k.pkl\") as input_fd:\n",
    "#     gensim_word_vectors = pickle.load(input_fd)\n",
    "# # gensim_word_vectors = KeyedVectors.load_word2vec_format(\"/home/haowu4/data/simple_finer/GoogleNews-vectors-negative300.combined_500k.txt\", binary=False)\n",
    "\n",
    "kbann = KBBiasTypeAnnotator(config, \"OntonoteType\")\n",
    "config[\"kba\"] = kbann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'award': 15,\n",
       " 'building': 10,\n",
       " 'event': 5,\n",
       " 'finance.currency': 9,\n",
       " 'law': 11,\n",
       " 'living_thing': 12,\n",
       " 'location': 0,\n",
       " 'medicine': 8,\n",
       " 'norpl': 4,\n",
       " 'organization': 6,\n",
       " 'person': 2,\n",
       " 'product': 14,\n",
       " u'software': 16,\n",
       " 'time': 1,\n",
       " 'title': 3,\n",
       " 'transportation.road': 13,\n",
       " 'work': 7}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_lex.lexeme_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def set_to_string(set_of_type):\n",
    "    labels = [\"/\" + s.replace(\".\", \"/\") for s in set_of_type]\n",
    "    return \",\".join(sorted(labels))\n",
    "\n",
    "\n",
    "def id(typs):\n",
    "    return [x for source, x in typs]\n",
    "\n",
    "\n",
    "def to_column_format(doc, use_views, type_map_function=id):\n",
    "    ret = \"\"\n",
    "    bios = defaultdict(lambda: \"O\")\n",
    "    typs = defaultdict(set)\n",
    "    for v, use_all in use_views:\n",
    "        view = doc.user_data[v]\n",
    "        for c in view.constituents:\n",
    "            for i in range(c.start, c.end):\n",
    "                if use_all and c.label2score:\n",
    "                    for t in c.label2score:\n",
    "                        typs[i].add((v, t))\n",
    "                else:\n",
    "                    # use best\n",
    "                    if c.best_label_name:\n",
    "                        typs[i].add((v, c.best_label_name))\n",
    "                bios[i] = \"I\"\n",
    "            bios[c.start] = \"B\"\n",
    "    for i, token in enumerate(doc):\n",
    "        w = token.text\n",
    "        if bios[i] == \"O\":\n",
    "            if len(typs[i]) == 0:\n",
    "                inc = \"%s\\t%s\\n\" % (w, \"O\")\n",
    "                ret += inc\n",
    "            else:\n",
    "                raise ValueError(\"O tag have types..\")\n",
    "        else:\n",
    "            if len(typs[i]) > 0:\n",
    "                types = type_map_function(typs[i])\n",
    "                inc = \"%s\\t%s-%s\\n\" % (w, bios[i], set_to_string(types))\n",
    "                ret += inc\n",
    "            else:\n",
    "                raise ValueError(\"B-I tag have no types..\")\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type building or body_part\n",
      "Not found type time or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type event or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type building or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type location or body_part\n",
      "Not found type person or body_part\n",
      "Not found type location or body_part\n",
      "Not found type person or body_part\n",
      "Not found type time or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type work or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type event or body_part\n",
      "Not found type location or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type time or body_part\n",
      "Not found type time or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type law or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type location or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type event or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type building or body_part\n",
      "Not found type product or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type time or body_part\n",
      "Not found type location or body_part\n",
      "Not found type location or body_part\n",
      "Not found type work or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type time or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type time or body_part\n",
      "Not found type person or body_part\n",
      "Not found type building or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type location or body_part\n",
      "Not found type location or body_part\n",
      "Not found type location or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type building or body_part\n",
      "Not found type location or body_part\n",
      "Not found type location or body_part\n",
      "Not found type law or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type location or body_part\n",
      "Not found type location or body_part\n",
      "Not found type person or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type building or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type organization or body_part\n",
      "Not found type work or body_part\n",
      "0 doc do not have coarse grain new view... \n",
      "Evaluating /tmp/big_one.label vs /tmp/eval_out/newdoc_out\n",
      "7587\n",
      "(5173.0, 7587.0, 7587.0)\n",
      "Strict F1 :\n",
      "P: 0.682\t R: 0.682\t F1: 0.682\n",
      "\n",
      "\n",
      "\n",
      "(8981.0, 8981.0, 10340.0, 11105.0)\n",
      "Losse Mirco :\n",
      "P: 0.869\t R: 0.809\t F1: 0.838\n",
      "\n",
      "\n",
      "\n",
      "(6710.416666666681, 6444.166666666667, 7587.0, 7587.0)\n",
      "Losse Marco F1 :\n",
      "P: 0.884\t R: 0.849\t F1: 0.867\n"
     ]
    }
   ],
   "source": [
    "def new_rule(typs):\n",
    "    ret = []\n",
    "    for source, x in typs:\n",
    "        if source == \"KBBiasType\" and x == \"organization.company\":\n",
    "#             pass\n",
    "            continue\n",
    "        if x  == \"ethnicity\":\n",
    "            x = \"people.ethnicity\"\n",
    "            ret.append(\"people.ethnicity\")\n",
    "    \n",
    "        if x == \"norpl\":\n",
    "            x = \"people.ethnicity\"\n",
    "#             ret.append(\"people\")\n",
    "            ret.append(\"people.ethnicity\")\n",
    "#             ret.append(\"location\")\n",
    "\n",
    "        if x == \"work\":\n",
    "#             ret.append(\"art\")\n",
    "            x = \"art\"\n",
    "            \n",
    "#         if x == \"transportation.road\":\n",
    "#             ret.append(\"location\")\n",
    "        if x == \"news_agency\":\n",
    "            ret.append(\"organization.company\")\n",
    "        if x == \"building\":\n",
    "            ret.append(\"location\")\n",
    "        if x == \"organization.sports_league\":\n",
    "            ret.append(\"organization.company\")\n",
    "            \n",
    "        ret.append(x)\n",
    "    return set(ret)\n",
    "\n",
    "\n",
    "VIEW_NAME = \"OntonoteType\"\n",
    "add_pred_view(y_figer_pred, figer_test, VIEW_NAME)\n",
    "add_pred_view(y_figer_gold_pred, figer_gold_test, VIEW_NAME)\n",
    "add_pred_view(y_new_doc_pred, new_doc_test, VIEW_NAME)\n",
    "\n",
    "def write_col_format(filename, figer_docs, use_types = [(\"OntonoteType\", False),\n",
    "                         (\"MRP-FINE\", False),\n",
    "                         (\"KBBiasType\", False),\n",
    "                         (\"GZFineType\",False),\n",
    "                         (\"MentionEntail\", False)]):\n",
    "    counter = 0\n",
    "\n",
    "    with codecs.open(filename, \"w\", \"utf-8\") as out:\n",
    "        for doc in figer_docs:\n",
    "            if \"OntonoteType\" not in doc.user_data:\n",
    "                counter += 1\n",
    "                doc.user_data[\"OntonoteType\"] = View()\n",
    "            kbann(doc)\n",
    "            meann(doc)\n",
    "            gz_fine_ann(doc)\n",
    "            mrp_ann(doc)\n",
    "            for a in non_default_annotators:\n",
    "                a(doc)\n",
    "#             use_types = [(\"OntonoteType\", False),\n",
    "#                          (\"KBBiasType\", False),\n",
    "#                          (\"GZFineType\",False),\n",
    "#                          (\"MentionEntail\", False)]\n",
    "            for t, _ in use_types:\n",
    "                if t not in doc.user_data:\n",
    "                    doc.user_data[t] = View()\n",
    "            s = to_column_format(doc, use_types, new_rule)\n",
    "    #         s = to_column_format(doc, [(\"OntonoteType\", False)])        \n",
    "            out.write(s)\n",
    "            out.write(\"\\n\")\n",
    "    print(\"%d doc do not have coarse grain new view... \" % counter)\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/newdoc_out\", annotated_ontonote_docs)\n",
    "\n",
    "ev_finer.eval_two_file( \"/tmp/big_one.label\",\n",
    "    \"/tmp/eval_out/newdoc_out\")\n",
    "\n",
    "# write_col_format(\"/home/haowu4/.py_cache/figer_gold_docs.out\", figer_gold_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "write_col_format(\"/tmp/eval_out/newdoc_out_coarse\", annotated_ontonote_docs, \n",
    "                             use_types = [\n",
    "                              (\"OntonoteType\", False),\n",
    "#                               (\"MRP-FINE\", False),                              \n",
    "#                               (\"KBBiasType\", False),\n",
    "#                               (\"GZFineType\",False),\n",
    "#                               (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "ev_finer.eval_two_file( \"/tmp/big_one.label\",\n",
    "    \"/tmp/eval_out/newdoc_out_coarse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "write_col_format(\"/tmp/eval_out/organized/outs/figer.out\", figer_docs)\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-noKB.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [(\"OntonoteType\", False),\n",
    "                             (\"MRP-FINE\", False),                              \n",
    "#                               (\"KBBiasType\", False),\n",
    "#                               (\"GZFineType\",False),\n",
    "                              (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-noHyp.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [(\"OntonoteType\", False),\n",
    "                              (\"MRP-FINE\", False),                              \n",
    "                              (\"KBBiasType\", False),\n",
    "                              (\"GZFineType\",False),\n",
    "#                               (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-noMenPt.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [(\"OntonoteType\", False),\n",
    "#                                 (\"MRP-FINE\", False),                              \n",
    "                              (\"KBBiasType\", False),\n",
    "                              (\"GZFineType\",False),\n",
    "                              (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-onlyKB.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [(\"OntonoteType\", False),\n",
    "#                              (\"MRP-FINE\", False),                              \n",
    "                              (\"KBBiasType\", False),\n",
    "                              (\"GZFineType\",False),\n",
    "#                               (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-onlyHyp.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [(\"OntonoteType\", False),\n",
    "#                               (\"MRP-FINE\", False),                              \n",
    "#                               (\"KBBiasType\", False),\n",
    "#                               (\"GZFineType\",False),\n",
    "                              (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-onlyMenPt.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [(\"OntonoteType\", False),\n",
    "                                (\"MRP-FINE\", False),                              \n",
    "#                               (\"KBBiasType\", False),\n",
    "#                               (\"GZFineType\",False),\n",
    "#                               (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-nofine.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [(\"OntonoteType\", False),\n",
    "#                                 (\"MRP-FINE\", False),                              \n",
    "#                               (\"KBBiasType\", False),\n",
    "#                               (\"GZFineType\",False),\n",
    "#                               (\"MentionEntail\", False)\n",
    "                             ])\n",
    "\n",
    "write_col_format(\"/tmp/eval_out/figer_docs-nocoarse.out\",\n",
    "                 figer_docs,\n",
    "                 use_types = [#(\"OntonoteType\", False),\n",
    "                                (\"MRP-FINE\", False),                              \n",
    "                              (\"KBBiasType\", False),\n",
    "                              (\"GZFineType\",False),\n",
    "                              (\"MentionEntail\", False)\n",
    "                             ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for c in figer_docs[0].user_data[\"fine_type_view\"].constituents:\n",
    "    print c.start, c.end, c.label2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "[figer_type_func(c) for c in figer_docs[0].user_data[\"gold_mention_view\"].constituents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GOLD_FIGER_PATH = \"/home/haowu4/codes/dataless_finer/python/eval_output/organized/gold/figer.xiang.label\"\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer.out\")\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer_docs-noKB.out\")\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer_docs-noHyp.out\")\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer_docs-noMenPt.out\")\n",
    "\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer_docs-onlyKB.out\")\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer_docs-onlyHyp.out\")\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer_docs-onlyMenPt.out\")\n",
    "\n",
    "ev_finer.eval_two_file( GOLD_FIGER_PATH,\n",
    "    \"/tmp/eval_out/figer_docs-nofine.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/eval_out/figer_docs.out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-aa8e5edd6818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_finer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mev_finer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_two_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"figer_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/eval_out/figer_docs.out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mev_finer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_two_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/haowu4/codes/dataless_finer/python/eval_output/xiang_ren_figer_gold.label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/eval_out/figer_docs.out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/haowu4/codes/dataless_finer/python/dfiner/eval/eval.pyc\u001b[0m in \u001b[0;36meval_two_file\u001b[0;34m(gold_file, pred_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mgold_labels_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mpred_labels_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# figer_labels_set = set()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/haowu4/codes/dataless_finer/python/dfiner/eval/eval.pyc\u001b[0m in \u001b[0;36mload_labels\u001b[0;34m(file, ty_maps)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/tmp/eval_out/figer_docs.out'"
     ]
    }
   ],
   "source": [
    "import dfiner.eval.eval as ev_finer\n",
    "reload(ev_finer)\n",
    "\n",
    "ev_finer.eval_two_file(config[\"figer_path\"], \"/tmp/eval_out/figer_docs.out\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "ev_finer.eval_two_file(\"/home/haowu4/codes/dataless_finer/python/eval_output/xiang_ren_figer_gold.label\", \"/tmp/eval_out/figer_docs.out\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# ev_finer.eval_two_file(\"/home/haowu4/codes/dataless_finer/python/eval_output/xiang_ren_figer_gold.label\",\n",
    "#                        \"/home/haowu4/codes/dataless_finer/python/eval_output/organized/abelation/figer_docs-noMenPt.out\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "# ev_finer.eval_two_file(config['figer_gold'], \"/home/haowu4/.py_cache/figer_gold_docs.out\")\n",
    "# print(\"----------------------------------------------------------------\")\n",
    "# ev_finer.eval_two_file(\"/home/haowu4/.py_cache/figer_coarse_gold.out\", \"/home/haowu4/.py_cache/figer_coarse_theirs.out\")\n",
    "\n",
    "# print(\"----------------------------------------------------------------\")\n",
    "# ev_finer.eval_two_file(\"/home/haowu4/.py_cache/figer_coarse_gold.out\", \"/home/haowu4/.py_cache/figer_docs_coarse.out\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "gensim_word_vectors = KeyedVectors.load_word2vec_format(\"/home/haowu4/data/simple_finer/GoogleNews-vectors-negative300.combined_500k.txt\", binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#gensim_word_vectors.most_similar(\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#gensim_word_vectors.most_similar_cosmul(\"Zarowsky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 33.32 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 301 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "\"A\" in gensim_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804253"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extended_w2vdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gensim_word_vectors.cached_most_sim = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'broadcast_program': 0.0017206521271561923,\n",
       " u'news_agency': 0.33268808878564976,\n",
       " u'organization.company': 0.3326450724824709,\n",
       " u'organization.sports_league': 0.33281713769518645,\n",
       " u'organization.sports_team': 4.30163031789048e-05,\n",
       " u'person.author': 8.60326063578096e-05}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbann.surface_to_type_dist[\"NBA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from dfiner.datastructures import View, Constituent\n",
    "from dfiner.annotators.fine_type_annotator import SynsetFineTyper\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from dfiner.types.finer_type_system import FinerTypeSystem\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def best_k_label(label2score, k):\n",
    "    min_v = sorted(label2score.values(), reverse=True)[k-1]\n",
    "    return [x for x in label2score if label2score[x] >= min_v]\n",
    "\n",
    "\n",
    "def embedding_of_phrase(gensim_w2v, doc, start, end):\n",
    "    base_vec = np.zeros(300)\n",
    "    counter = 0.0\n",
    "    for w in doc[start: end]:\n",
    "        w = w.text\n",
    "        if w == \"UW\":\n",
    "            w = \"University\"\n",
    "        if w.lower() in stopWords:\n",
    "            continue            \n",
    "        if w in gensim_w2v:\n",
    "            b = gensim_w2v[w]\n",
    "            base_vec += b\n",
    "            counter += 1.0\n",
    "    return base_vec\n",
    "\n",
    "\n",
    "class MentionEntailmentAnnotator(object):\n",
    "    TYPE_NAME = \"MentionEntail\"\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 gensim_w2v,\n",
    "                 min_cosine=0.4,\n",
    "                 mention_view =\"OntonoteType\"\n",
    "                 ):\n",
    "        self.typer = SynsetFineTyper(config)\n",
    "        self.min_cosine = min_cosine\n",
    "        self.gensim_w2v = gensim_w2v\n",
    "        self.type_system = FinerTypeSystem.load_type_system(config)\n",
    "        self.mention_view = mention_view\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        new_view = View()\n",
    "        view = doc.user_data[self.mention_view]\n",
    "        for constituent in view.constituents:\n",
    "            start = constituent.start\n",
    "            end = constituent.end\n",
    "            types = set()\n",
    "            max_sim = defaultdict(float)\n",
    "            mention_vec = embedding_of_phrase(self.gensim_w2v, doc, start, end)\n",
    "            mention_vec = embedding_of_phrase(self.gensim_w2v, doc, 0, len(doc))\n",
    "            \n",
    "            log_ = False\n",
    "            for w in doc[max(start-1, 0): end]:\n",
    "#                 print \"Searching ... \", w\n",
    "                \n",
    "                if w.text == \"UW\":\n",
    "                    text_ = \"University\"\n",
    "                    log_ = True\n",
    "                else:\n",
    "                    text_ = w.text\n",
    "                    \n",
    "                for x in wn.synsets(text_):\n",
    "                    if x.pos() == \"n\":\n",
    "                        def_doc = nlp.make_doc(x.definition())\n",
    "                        def_vec = embedding_of_phrase(self.gensim_w2v, def_doc, 0, len(def_doc))\n",
    "                        sim_score = cosine_similarity(def_vec.reshape(1,-1), mention_vec.reshape(1,-1))[0,0]                        \n",
    "                        tps_w = self.typer.get_fine_types(\"%d_n\" % x.offset())\n",
    "#                         if log_:\n",
    "#                             print(tps_w)\n",
    "                        for t in tps_w:\n",
    "                            types.add(t)\n",
    "                            max_sim[t] = max(max_sim[t], sim_score)\n",
    "#                         print(x)\n",
    "#                         print(x.definition())\n",
    "#                         print(sim_score)\n",
    "#                         print(types)\n",
    "#                         print(\" \")\n",
    "            if len(constituent.label2score) == 0:\n",
    "                continue\n",
    "                \n",
    "#             kk = max(self.trust_k, len(constituent.label2score))\n",
    "#             print(kk)\n",
    "#             ls = set(best_k_label(constituent.label2score, kk))\n",
    "#             if log_:\n",
    "#                 print(types)\n",
    "#                 print(max_sim)\n",
    "#             print(\"111\",ls)\n",
    "            ls = constituent.label2score.keys()\n",
    "            mx_label2score = {}\n",
    "#             print(\"\\n\"*3)\n",
    "#             print(\"Fine types\", types)\n",
    "#             print(\"Coarse types\", ls)\n",
    "            for fine_type in types:\n",
    "                for coarse_type in ls:\n",
    "                    try:\n",
    "#                         print(\"Checking [%s],[%s]\" % (fine_type, coarse_type) )\n",
    "                        check = self.type_system.a_belongs_to_b(fine_type, coarse_type)\n",
    "#                         print check\n",
    "                        if check:\n",
    "#                             print(\"!!! Checking %s,%s\" % (fine_type, coarse_type) )\n",
    "                            if max_sim[fine_type] > self.min_cosine:\n",
    "                                mx_label2score[fine_type] = max_sim[fine_type] \n",
    "                    except KeyError:\n",
    "                        print(\"Not found type %s or %s\" % (coarse_type, fine_type))\n",
    "                        continue\n",
    "#             print(mx_label2score)\n",
    "#             if log_:\n",
    "#                 print(mx_label2score)\n",
    "#                 print(max_sim)\n",
    "            if len(mx_label2score) > 0:\n",
    "                c = Constituent(start,\n",
    "                                end,\n",
    "                                self.TYPE_NAME,\n",
    "                                label2score=mx_label2score)\n",
    "                new_view.add_constituent(c)\n",
    "        doc.user_data[self.TYPE_NAME] = new_view\n",
    "\n",
    "meann = MentionEntailmentAnnotator(config, gensim_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# kbann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# gensim_word_vectors.most_similar(\"Cougars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# kbann.surface_to_type_dist[u'AAAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import gzip\n",
    "import json\n",
    "from dfiner.datastructures import View, Constituent\n",
    "from dfiner.types.finer_type_system import FinerTypeSystem\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class KBBiasTypeAnnotator(object):\n",
    "\n",
    "    TYPE_NAME = \"KBBiasType\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_surface_to_typedist(fname):\n",
    "        ret = {}\n",
    "        with gzip.open(fname, 'rb') as zf:\n",
    "            reader = codecs.getreader(\"utf-8\")\n",
    "            contents = reader(zf)\n",
    "            for line in contents:\n",
    "                obj = json.loads(line)\n",
    "                ret[obj['surface']] = obj[\"type_dist\"]\n",
    "        return ret\n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 mention_view=\"coarse_type\"):\n",
    "        # surface_to_type_dist maps\n",
    "        #       (surface, coarse type) => fine type to fine type.\n",
    "        self.surface_to_type_dist = self.load_surface_to_typedist(\n",
    "            config[\"mention_to_type_dist\"])\n",
    "        self.coarse_view_name = mention_view\n",
    "        self.config = config\n",
    "        self.type_system = FinerTypeSystem.load_type_system(config)\n",
    "        # self.cities_names = set(pd.read_csv(config[\"common_us_city_path\"], names=[\"Name\", \"State\", \"Pop\"])[\"Name\"].tolist())\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        new_view = View()\n",
    "        view = doc.user_data[self.coarse_view_name]\n",
    "        for constituent in view.constituents:\n",
    "            start = constituent.start\n",
    "            end = constituent.end\n",
    "            coarse_type = constituent.best_label_name\n",
    "            surface = doc[start:end].text\n",
    "            # if surface in self.cities_names:\n",
    "            #         c = Constituent(start,\n",
    "            #                         end,\n",
    "            #                         self.TYPE_NAME,\n",
    "            #                         label2score={\"location.city\": 1.0})\n",
    "                    \n",
    "                    # new_view.add_constituent(c)\n",
    "            try:\n",
    "                type_dist = self.surface_to_type_dist[surface]\n",
    "                fine_type_name = self.pick_fine_type_or_none(type_dist,\n",
    "                                                             coarse_type)\n",
    "                if fine_type_name:\n",
    "                    c = Constituent(start,\n",
    "                                    end,\n",
    "                                    self.TYPE_NAME,\n",
    "                                    label2score={fine_type_name: 1.0})\n",
    "                    new_view.add_constituent(c)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        doc.user_data[self.TYPE_NAME] = new_view\n",
    "\n",
    "    def pick_fine_type_or_none(self, type_dist, coarse_type):\n",
    "        consistent_types = {}\n",
    "        max_prob = 0.0\n",
    "        rescale = 0.0\n",
    "        best_type = None\n",
    "        for t in type_dist:\n",
    "            if self.type_system.a_belongs_to_b(t, coarse_type):\n",
    "                p = type_dist[t]\n",
    "                rescale += p\n",
    "                if p > max_prob:\n",
    "                    max_prob = p\n",
    "                    best_type = t\n",
    "                consistent_types[t] = p\n",
    "\n",
    "        if len(consistent_types) == 0:\n",
    "            return None\n",
    "\n",
    "        if len(consistent_types) == 1:\n",
    "            if max_prob > 0.4:\n",
    "                return best_type\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        sorted_entry = sorted(consistent_types.keys(),\n",
    "                              key=lambda x: consistent_types[x],\n",
    "                              reverse=True)\n",
    "        second_best_key = sorted_entry[1]\n",
    "\n",
    "        if (max_prob - consistent_types[second_best_key]) / rescale > 0.8:\n",
    "            return best_type\n",
    "\n",
    "        best_key = sorted_entry[1]\n",
    "        if best_key == \"organization.company\":\n",
    "            if consistent_types[second_best_key] > 0.35:\n",
    "                return second_best_key\n",
    "\n",
    "        if second_best_key == \"organization.company\":\n",
    "            return best_key\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "kbann = KBBiasTypeAnnotator(config, \"OntonoteType\")\n",
    "config[\"kba\"] = kbann\n",
    "kbann.surface_to_type_dist[\"Xinhua News Agency\"] = {'news_agency': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import gzip\n",
    "import json\n",
    "from dfiner.datastructures import View, Constituent\n",
    "from dfiner.types.finer_type_system import FinerTypeSystem\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class GZFineTypeAnnotator(object):\n",
    "\n",
    "    TYPE_NAME = \"GZFineType\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_gzs(base_folder, exclude = {}):\n",
    "        surface_to_type = {}\n",
    "        \n",
    "        for gz_entry in os.listdir(base_folder):\n",
    "            if gz_entry in exclude:\n",
    "                continue\n",
    "            with open(os.path.join(base_folder,gz_entry)) as inp:\n",
    "                for line in inp:\n",
    "                    line = line.strip()\n",
    "                    if line not in surface_to_type:\n",
    "                        surface_to_type[line] = gz_entry\n",
    "                    else:\n",
    "                        surface_to_type[line] = None\n",
    "        return surface_to_type\n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 mention_view=\"OntonoteType\"):\n",
    "        # surface_to_type_dist maps\n",
    "        #       (surface, coarse type) => fine type to fine type.\n",
    "        self.surface_to_type = self.load_gzs(\n",
    "            config[\"fine_gz_base\"])\n",
    "        self.coarse_view_name = mention_view\n",
    "        self.config = config\n",
    "        self.type_system = FinerTypeSystem.load_type_system(config)\n",
    "        # self.cities_names = set(pd.read_csv(config[\"common_us_city_path\"], names=[\"Name\", \"State\", \"Pop\"])[\"Name\"].tolist())\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        new_view = View()\n",
    "        view = doc.user_data[self.coarse_view_name]\n",
    "        for constituent in view.constituents:\n",
    "            start = constituent.start\n",
    "            end = constituent.end\n",
    "            coarse_type = constituent.best_label_name\n",
    "            surface = doc[start:end].text\n",
    "\n",
    "            try:\n",
    "                typ = self.surface_to_type[surface]\n",
    "                if typ:\n",
    "                    if self.type_system.a_belongs_to_b(typ, coarse_type):\n",
    "                        c = Constituent(start,\n",
    "                                        end,\n",
    "                                        self.TYPE_NAME,\n",
    "                                        label2score={typ: 1.0})\n",
    "                        new_view.add_constituent(c)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        doc.user_data[self.TYPE_NAME] = new_view\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "config[\"fine_gz_base\"] = \"/home/haowu4/data/simple_finer/fine_type_gazetteers\"\n",
    "gz_fine_ann = GZFineTypeAnnotator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organization.sports_league'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gz_fine_ann.surface_to_type[\"NFL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import gzip\n",
    "import json\n",
    "from dfiner.datastructures import View, Constituent\n",
    "from dfiner.types.finer_type_system import FinerTypeSystem\n",
    "import pandas as pd\n",
    "import os\n",
    "import cPickle as pickle\n",
    "class MentionRegexPatternTypeAnnotator(object):\n",
    "\n",
    "    TYPE_NAME = \"MRP-FINE\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_patterns(path):\n",
    "        with open(path, \"rb\") as inp:\n",
    "            return pickle.load(inp)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_all_pattern(tokens, ks = range(1,5)):\n",
    "        for i, w in enumerate(tokens):\n",
    "            word_before = i\n",
    "            for k in ks:            \n",
    "                word_after = len(tokens) - i - k\n",
    "                if word_after < 0:\n",
    "                    break\n",
    "                yield (word_before,tuple(tokens[i:i+k])  , word_after)\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 mention_view=\"OntonoteType\"):\n",
    "        # surface_to_type_dist maps\n",
    "        #       (surface, coarse type) => fine type to fine type.\n",
    "        self.pattern_db = self.load_patterns(\n",
    "            config[\"pattern_db_path\"])\n",
    "        self.coarse_view_name = mention_view\n",
    "        self.config = config\n",
    "        self.type_system = FinerTypeSystem.load_type_system(config)\n",
    "        # self.cities_names = set(pd.read_csv(config[\"common_us_city_path\"], names=[\"Name\", \"State\", \"Pop\"])[\"Name\"].tolist())\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        new_view = View()\n",
    "        view = doc.user_data[self.coarse_view_name]\n",
    "        for constituent in view.constituents:\n",
    "            start = constituent.start\n",
    "            end = constituent.end\n",
    "            coarse_type = constituent.best_label_name\n",
    "            \n",
    "            surface = [x.text for x in doc[start:end]]\n",
    "            \n",
    "            if len(surface) == 1 and surface[0] == \"UW\":\n",
    "                surface = [\"University\", \"of\" ,\"Washington\"]\n",
    "#             all_pat = extract_all_pattern(surface)\n",
    "\n",
    "            candidate_types = defaultdict(float)\n",
    "\n",
    "            for pat in self.extract_all_pattern(surface):\n",
    "                if pat in self.pattern_db:\n",
    "                    for candidate, sup in self.pattern_db[pat]:\n",
    "                        try:\n",
    "                            if self.type_system.a_belongs_to_b(candidate, coarse_type):\n",
    "                                candidate_types[candidate] += sup\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "            if len(candidate_types) > 0:\n",
    "#                 print(surface)\n",
    "                c = Constituent(start,\n",
    "                                end,\n",
    "                                self.TYPE_NAME,\n",
    "                                label2score=candidate_types)\n",
    "\n",
    "                new_view.add_constituent(c)\n",
    "\n",
    "        doc.user_data[self.TYPE_NAME] = new_view\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "config[\"pattern_db_path\"] = \"/tmp/pat_dump\"\n",
    "\n",
    "mrp_ann = MentionRegexPatternTypeAnnotator(config)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(3):\n",
    "        mrp_ann.pattern_db[(i,(\"Inc.\",),j)].add(('organization.company', 0.014499239237447418))\n",
    "        mrp_ann.pattern_db[(i,(\"LLC\",),j)].add(('organization.company', 0.014499239237447418))\n",
    "        mrp_ann.pattern_db[(i,(\"Co.\",),j)].add(('organization.company', 0.014499239237447418))\n",
    "        mrp_ann.pattern_db[(i,(\"AG\",),j)].add(('organization.company', 0.014499239237447418))\n",
    "        mrp_ann.pattern_db[(i,(\"GmbH\",),j)].add(('organization.company', 0.014499239237447418))\n",
    "        mrp_ann.pattern_db[(i,(\"Corp.\",),j)].add(('organization.company', 0.014499239237447418))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('Viatech',), 1)\n",
      "(0, ('Viatech', 'Inc.'), 0)\n",
      "(1, ('Inc.',), 0)\n",
      "organization.company\n"
     ]
    }
   ],
   "source": [
    "for k in mrp_ann.extract_all_pattern([\"Viatech\", \"Inc.\"]):\n",
    "    print k\n",
    "    if k in mrp_ann.pattern_db:\n",
    "        for candidate, sup in mrp_ann.pattern_db[k]:\n",
    "            print candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mrp_ann' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-caeb66fb5723>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmrp_ann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern_db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mrp_ann' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Py2-finer",
   "language": "python",
   "name": "finer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
