{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, OrderedDict\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "class Mention(object):\n",
    "    def __init__(self, start, end, label, tokens, doc_id = \"\"):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.label = label\n",
    "        self.tokens = tokens\n",
    "        self.surface = [tokens[i] for i in range(start,end)]\n",
    "        self.ext_surface = [tokens[i] for i in range(max(start-1,0),min(end+1,len(tokens)))]\n",
    "        self.doc_id = doc_id\n",
    "        doc = nlp.tokenizer.tokens_from_list(tokens)\n",
    "        nlp.tagger(doc)\n",
    "        nlp.parser(doc)\n",
    "        self.doc = doc\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        o = [(\"start\",self.start),\n",
    "            (\"end\",self.end),\n",
    "            (\"surface\",self.surface), \n",
    "            (\"label\",self.label),\n",
    "            (\"doc_id\",self.doc_id),\n",
    "            (\"tokens\",self.tokens)]\n",
    "        \n",
    "        return json.dumps(OrderedDict(o), indent=4)\n",
    "        \n",
    "def read_figer(file = \"/home/haowu4/codes/dataless_finer/eval_figer/data/gold_cleaned_figer_test.label\"):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with codecs.open(file, \"r\", \"utf-8\") as input:\n",
    "        for i, line in enumerate(input):\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                # New sentence\n",
    "                if len(tokens) == 0:\n",
    "                    continue\n",
    "                sent = get_sentence(tokens, labels)\n",
    "                for s in sent:\n",
    "                    sentences.append(s)\n",
    "                tokens = []\n",
    "                labels = []\n",
    "                continue\n",
    "            line = line.split(\"\\t\")\n",
    "            word, label = line[0], line[1]\n",
    "            tokens.append(word)\n",
    "            labels.append(label)\n",
    "            \n",
    "    if len(tokens) == 0:\n",
    "        return sentences\n",
    "\n",
    "    sent = get_sentence(tokens, labels)\n",
    "    sentences.append(sent)\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    return sentences\n",
    "\n",
    "    \n",
    "        \n",
    "def get_sentence(tokens, labels):\n",
    "    entities = []\n",
    "    current_labels = \"\"\n",
    "    current_start = 0\n",
    "    in_mention = False\n",
    "    for i, (t, lab_str) in enumerate(zip(tokens, labels)):\n",
    "        if lab_str.startswith(\"B\"):\n",
    "            if in_mention:\n",
    "                entities.append(Mention(current_start, i, current_labels, tokens))\n",
    "                in_mention = False\n",
    "            current_start = i\n",
    "            current_labels = lab_str.split(\"-\")[1]\n",
    "            in_mention = True\n",
    "\n",
    "        if lab_str == \"O\":\n",
    "            if in_mention:\n",
    "                entities.append(Mention(current_start, i, current_labels, tokens))\n",
    "                in_mention = False\n",
    "   \n",
    "    if in_mention:\n",
    "        entities.append(Mention(current_start, len(labels), current_labels, tokens))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def load_ontonotes(file):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with codecs.open(file, \"r\", \"utf-8\") as input:\n",
    "        for i, line in enumerate(input):\n",
    "            if i < 1:\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                # New sentence\n",
    "                if len(tokens) == 0:\n",
    "                    continue\n",
    "                sent = get_sentence(tokens, labels)\n",
    "                for s in sent:\n",
    "                    sentences.append(s)\n",
    "                tokens = []\n",
    "                labels = []\n",
    "                continue\n",
    "            line = line.split(\"\\t\")\n",
    "            word, label = line[5], line[0]\n",
    "            tokens.append(word)\n",
    "            labels.append(label)\n",
    "            \n",
    "    if len(tokens) == 0:\n",
    "        return sentences\n",
    "\n",
    "    sent = get_sentence(tokens, labels)\n",
    "    sentences.append(sent)\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def load_all_data(base_folder):\n",
    "    ret = []\n",
    "    for f in os.listdir(base_folder):\n",
    "        fn = os.path.join(base_folder,f)\n",
    "        ms = load_ontonotes(fn)\n",
    "        for m in ms:\n",
    "            m.doc_id = f\n",
    "            ret.append(m)\n",
    "    return ret\n",
    "\n",
    "class Lexicon(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.curr = 0\n",
    "        self.m = {}\n",
    "        self.counter = defaultdict(int)\n",
    "        self.counter_per_type = defaultdict(lambda:defaultdict(int))\n",
    "    \n",
    "    def see_feature(self, f, t = None):\n",
    "        self.counter[f] += 1\n",
    "        if t:\n",
    "            self.counter_per_type[f][t] += 1\n",
    "        if f not in self.m:\n",
    "            self.m[f] = self.curr\n",
    "            self.curr += 1\n",
    "            \n",
    "    \n",
    "    def prune(self, min_support):\n",
    "        self.curr = 0\n",
    "        self.m = {}\n",
    "        for k in self.counter:\n",
    "            if self.counter[k] > min_support:\n",
    "                self.m[k] = self.curr\n",
    "                self.curr += 1\n",
    "    \n",
    "    def getOrNegOne(self, f):\n",
    "        if f in self.m:\n",
    "            return self.m[f]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def getOneHot(self, f):\n",
    "        ret = np.zeros((self.curr))\n",
    "        if f in self.m:\n",
    "            ret[self.m[f]] = 1.0\n",
    "        return ret\n",
    "    \n",
    "def loadW2V(w2v_file, allowed = None):\n",
    "    ret = {}\n",
    "    err = 0\n",
    "    with codecs.open(w2v_file, \"r\" , 'utf-8') as input:\n",
    "        for line in input:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            try:\n",
    "                w,vec = line.split(\"\\t\")\n",
    "#                 w,vec = line[0], line[1:]\n",
    "            except ValueError:\n",
    "#                 print(line)\n",
    "                err += 1\n",
    "                continue\n",
    "            if allowed is not None and w in allowed:\n",
    "                vec = [float(v) for v in vec.split(\" \")]\n",
    "                ret[w] = np.array(vec)\n",
    "    print(\"%d line failed\" % err)\n",
    "    return ret\n",
    "\n",
    "def generate_vecs(objs, \n",
    "                  typ_function,\n",
    "                  feature_func,\n",
    "                  dense_real_vec_features = [],\n",
    "                  lex = None,\n",
    "                  type_lex = None\n",
    "                 ):\n",
    "    len_x = len(objs)\n",
    "    \n",
    "    if lex is None:\n",
    "        lex = Lexicon()\n",
    "        for x in objs:\n",
    "            for ff in feature_func:\n",
    "                for k,v in ff(x.doc, x.start, x.end):\n",
    "                    lex.see_feature(k, typ_function(x))\n",
    "        lex.prune(7)\n",
    "\n",
    "    if type_lex is None:\n",
    "        type_lex = Lexicon()\n",
    "        for x in objs:\n",
    "            y = typ_function(x)\n",
    "            type_lex.see_feature(y)\n",
    "\n",
    "    \n",
    "    \n",
    "    row_ids = []\n",
    "    col_ids = []\n",
    "    vs = []\n",
    "    \n",
    "    dense_vecs = []\n",
    "    \n",
    "    ys = []\n",
    "    \n",
    "    for i, x in enumerate(objs):\n",
    "        for ff in feature_func:\n",
    "            for k,v in ff(x.doc, x.start, x.end):\n",
    "                idx = lex.getOrNegOne(k)\n",
    "                if idx > -1:\n",
    "                    row_ids.append(i)\n",
    "                    col_ids.append(idx)\n",
    "                    vs.append(v)\n",
    "                    \n",
    "        if len(dense_real_vec_features) > 0:\n",
    "            ds = []\n",
    "            for dff in dense_real_vec_features:\n",
    "                v  = dff(x)\n",
    "                ds.append(v)\n",
    "#             print(len(ds))\n",
    "            denv = np.hstack((ds))\n",
    "            dense_vecs.append(denv)\n",
    "            \n",
    "        ys.append(type_lex.getOrNegOne(typ_function(x)))\n",
    "        \n",
    "    sp = (len_x, lex.curr)\n",
    "    print(sp)\n",
    "    xs = coo_matrix((vs, (row_ids, col_ids)), shape=sp)\n",
    "    print(\"dense_vecs[0].shape\", dense_vecs[0].shape)\n",
    "    dense_vecs = np.vstack(dense_vecs)\n",
    "    print(\"dense_vecs.shape\", dense_vecs.shape)\n",
    "    if len(dense_real_vec_features) > 0:\n",
    "        print(\"shapes : \", xs.shape, dense_vecs.shape, )\n",
    "        xs = hstack((xs, dense_vecs))\n",
    "    return lex, type_lex, xs.tocsr(), np.asarray(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141634\n",
      "159298 mention loaded\n",
      "52467 word loaded\n",
      "1 line failed\n",
      "41459 words have w2v\n"
     ]
    }
   ],
   "source": [
    "train_mentions= load_all_data(\"/home/haowu4/data/ontonotes_ner/ColumnFormat/TrainAndDev/\")\n",
    "print(len(train_mentions))\n",
    "\n",
    "# conll_train = load_all_data(\"/home/haowu4/data/conll_ner/\")\n",
    "# for x in conll_train:\n",
    "#     train_mentions.append(x)\n",
    "\n",
    "# dev_mentions= load_all_data(\"/home/haowu4/data/ontonotes_ner/ColumnFormat/Dev/\")\n",
    "test_mentions= load_all_data(\"/home/haowu4/data/ontonotes_ner/ColumnFormat/Test/\")\n",
    "figer_datas = read_figer()\n",
    "\n",
    "VOCABS = set()\n",
    "mention_counter = 0\n",
    "for mention in [train_mentions,test_mentions, figer_datas]:\n",
    "    for men in mention:\n",
    "        mention_counter += 1\n",
    "        for t in men.tokens:\n",
    "            VOCABS.add(t)\n",
    "print(\"%d mention loaded\" % mention_counter)\n",
    "\n",
    "print(\"%d word loaded\" % len(VOCABS))\n",
    "\n",
    "w2vdict=loadW2V(\"/home/haowu4/data/autoextend/GoogleNews-vectors-negative300.combined_500k.txt\", VOCABS)\n",
    "print(\"%d words have w2v\" % len(w2vdict))\n",
    "\n",
    "default_w2v_mean = np.mean(list(w2vdict.values()), axis=0)\n",
    "default_w2v_zero = np.zeros(default_w2v_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "# with open(\"/home/haowu4/data/ontonotes_model/w2vdict\",'wb') as o:\n",
    "#     pickle.dump(w2vdict,o)\n",
    "    \n",
    "# with open(\"/home/haowu4/data/ontonotes_model/mean_w2v\",'wb') as o:\n",
    "#     pickle.dump(default_w2v_mean,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define features:\n",
    "\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# lmtzr = WordNetLemmatizer().lemmatize\n",
    "\n",
    "\n",
    "def word_shape_func(text):\n",
    "    text = re.sub(\"[a-z]+\", \"a\" ,text)\n",
    "    text = re.sub(\"[A-Z]+\", \"A\" ,text)\n",
    "    text = re.sub(\"[0-9]+\", \"0\" ,text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "class FeatureFunc(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, original_func):\n",
    "        decorator_self = self\n",
    "        def wrappee( *args, **kwargs):\n",
    "            for feat in original_func(*args,**kwargs):\n",
    "                yield (\"%s=%s\" % (self.name, feat), 1.0) \n",
    "        return wrappee\n",
    "\n",
    "class RealFeatureFunc(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, original_func):\n",
    "        decorator_self = self\n",
    "        def wrappee( *args, **kwargs):\n",
    "            for feat,v in original_func(*args,**kwargs):\n",
    "                yield (\"%s=%s\" % (self.name, feat), v)\n",
    "        return wrappee\n",
    "\n",
    "@FeatureFunc(\"dep_feature\")\n",
    "def mention_details(doc, start, end):\n",
    "    heads = [token.head for token in doc[start:end]]\n",
    "    deps = [list(token.children) for token in doc[start:end]]\n",
    "    for token, head, children in zip(doc[start:end], heads, deps):\n",
    "        if not (head.i >= start and head.i < end):            \n",
    "            yield \"<-%s- %s\" % (token.dep_, head.lemma_)\n",
    "            yield \"<-%s\" % (head.lemma_)\n",
    "        for child in children:\n",
    "            if not (child.i >= start and child.i < end):\n",
    "                yield  \"-%s-> %s\" % (child.dep_,child.lemma_)\n",
    "                yield  \"-> %s\" % (child.lemma_)\n",
    "\n",
    "def word_before(pos):\n",
    "    @FeatureFunc(\"word_before\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(max(start-pos,0), start):\n",
    "            yield doc[i].text\n",
    "#             yield word_shape_func(doc[i].text)\n",
    "    return f\n",
    "\n",
    "def word_before_loc(pos):\n",
    "    @FeatureFunc(\"word_before\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(max(start-pos,0), start):\n",
    "            yield \"%d-%s\" % (start - i,doc[i].text)\n",
    "    return f\n",
    "\n",
    "def word_before_lemma(pos):\n",
    "    @FeatureFunc(\"word_before_lemma\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(max(start-pos,0), start):\n",
    "            yield doc[i].lemma_\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_after(pos):\n",
    "    @FeatureFunc(\"word_after\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(end, min(end+pos,len(doc))):\n",
    "            yield doc[i].text\n",
    "#             yield word_shape_func(doc[i].text)\n",
    "    return f\n",
    "\n",
    "def word_after_loc(pos):\n",
    "    @FeatureFunc(\"word_after_loc\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(end, min(end+pos,len(doc))):\n",
    "            yield \"%d-%s\" % (i - end,doc[i].text)\n",
    "    return f\n",
    "\n",
    "\n",
    "def word_after_lemma(pos):\n",
    "    @FeatureFunc(\"word_after_lemma\")\n",
    "    def f(doc, start, end):\n",
    "        for i in range(end, min(end+pos,len(doc))):\n",
    "            yield doc[i].lemma_\n",
    "        \n",
    "    return f\n",
    "\n",
    "@FeatureFunc(\"wim\")\n",
    "def word_in_mention(doc, start, end):\n",
    "    for x in doc[start:end]:\n",
    "        yield x.text\n",
    "#         yield word_shape_func(x.text)\n",
    "\n",
    "@FeatureFunc(\"wim_lemma\")\n",
    "def word_in_mention_lemma(doc, start, end):\n",
    "    for x in doc[start:end]:\n",
    "        yield x.lemma_\n",
    "        \n",
    "\n",
    "@FeatureFunc(\"wim_loc\")\n",
    "def word_in_mention_loc(doc, start, end):\n",
    "    for i,x in enumerate(doc[start:end]):\n",
    "        yield \"f%d-%s\" % (i,x.text)\n",
    "        yield \"b%d-%s\" % ((end-start) - i,x.text)\n",
    "        \n",
    "@FeatureFunc(\"wim_loc_lemma\")\n",
    "def word_in_mention_loc_lemma(doc, start, end):\n",
    "    for i,x in enumerate(doc[start:end]):\n",
    "        x = x.lemma_\n",
    "        yield \"f%d-%s\" % (i,x)\n",
    "        yield \"b%d-%s\" % ((end-start)-i,x)\n",
    "\n",
    "\n",
    "\n",
    "# @FeatureFunc(\"wim_ext\")\n",
    "# def wim_ext(doc, start, end):\n",
    "#     for x in mention.ext_surface:\n",
    "#         yield x\n",
    "#         yield word_shape_func(x)\n",
    "    \n",
    "# @FeatureFunc(\"wim_ext_lemma\")\n",
    "# def wim_ext_lemma(doc, start, end):\n",
    "#     for x in mention.ext_surface:\n",
    "#         x = lmtzr(x)\n",
    "#         yield x\n",
    "    \n",
    "@FeatureFunc(\"wim_bigram\")\n",
    "def wim_bigram(doc, start, end):\n",
    "    for i,x in zip(doc[start:end-1], doc[start+1:end]):\n",
    "        yield \"%s-%s\" % (i.text, x.text)\n",
    "        \n",
    "@FeatureFunc(\"wim_bigram_lemma\")\n",
    "def wim_bigram_lemma(doc, start, end):\n",
    "    lms = [x.lemma_ for x in doc[start:end]]\n",
    "    for i,x in zip(lms[:-1], lms[1:]):\n",
    "        yield \"%s-%s\" % (i,x)\n",
    "\n",
    "\n",
    "        \n",
    "@FeatureFunc(\"word_shape\")\n",
    "def word_shape(doc, start, end):\n",
    "    t = \" \".join([x.text for x in doc[start:end]])\n",
    "    return [word_shape_func(t)]\n",
    "        \n",
    "@FeatureFunc(\"length\")\n",
    "def mention_length(doc, start, end):\n",
    "    return [\"%d\" % (end-start)]\n",
    "        \n",
    "@FeatureFunc(\"prefix\")\n",
    "def prefix(doc, start, end):\n",
    "    for w in doc[start:end]:\n",
    "        for i in range(3, min(5, len(w.text))):\n",
    "            yield w.text[:i]\n",
    "        \n",
    "@FeatureFunc(\"surfix\")\n",
    "def postfix(doc, start, end):\n",
    "    for w in doc[start:end]:\n",
    "        for i in range(3, min(5, len(w.text))):\n",
    "            yield w.text[-i:]\n",
    "\n",
    "@FeatureFunc(\"bias\")\n",
    "def CONSTANT_BIAS(doc, start, end):\n",
    "    return [\"bias\"]\n",
    "\n",
    "# def gazzarteer(gas):\n",
    "#     @FeatureFunc(\"gazzarteer\")\n",
    "#     def gazzarteer_wrappee(mention):\n",
    "#         for gz_name in gas:\n",
    "#             if mention.surface:\n",
    "#                 return\n",
    "\n",
    "def getOrDefault(m, k, d):\n",
    "    if k in m:\n",
    "        return m[k]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "def w2vBefore(ws, default_w2v, pos = 4):    \n",
    "    def wrappee(mention):\n",
    "        words = []\n",
    "        for i in range(mention.start-pos, mention.start):\n",
    "            if i < 0:\n",
    "                words.append(np.zeros(default_w2v.shape))\n",
    "            else:\n",
    "                words.append(getOrDefault(ws,mention.tokens[i],default_w2v))\n",
    "        words.append(np.mean(words, axis=0))\n",
    "        return np.hstack(words)\n",
    "    return wrappee\n",
    "\n",
    "def w2vAfrer(ws, default_w2v, pos = 4):    \n",
    "    def wrappee(mention):\n",
    "        words = []\n",
    "        for i in range(mention.end + 1, mention.end + pos + 1):\n",
    "            if i >= len(mention.tokens):\n",
    "                words.append(np.zeros(default_w2v.shape))\n",
    "            else:\n",
    "                words.append(getOrDefault(ws,mention.tokens[i],default_w2v))\n",
    "        words.append(np.mean(words, axis=0))\n",
    "        return np.hstack(words)\n",
    "\n",
    "    return wrappee\n",
    "\n",
    "def w2vMention(ws, default_w2v):    \n",
    "    def wrappee(mention):\n",
    "        if len(mention.surface) == 0:\n",
    "            print(mention)\n",
    "        ms = [getOrDefault(ws,w,default_w2v) for w in mention.surface]\n",
    "        return np.mean(ms, axis=0)\n",
    "    return wrappee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_mentions = [x for x in train_mentions if len(x.tokens) > 5]\n",
    "\n",
    "features = [CONSTANT_BIAS,\n",
    "            mention_details,\n",
    "            word_before(2), word_before_lemma(2),\n",
    "            word_after(2), word_after_lemma(2),\n",
    "            word_in_mention, word_in_mention_lemma,\n",
    "            word_in_mention_loc, word_in_mention_loc_lemma,\n",
    "            wim_bigram, wim_bigram_lemma,\n",
    "#             wim_ext, wim_ext_lemma,\n",
    "            word_shape,\n",
    "#             mention_length,\n",
    "            prefix,\n",
    "            postfix,\n",
    "           ]\n",
    "\n",
    "default_w2v = default_w2v_mean\n",
    "\n",
    "dense_feature = [ #w2vBefore(w2vdict, default_w2v),\n",
    "                  #w2vAfrer(w2vdict, default_w2v),\n",
    "                  w2vMention(w2vdict, default_w2v)]\n",
    "\n",
    "def typ_func(m):\n",
    "    if m.label == \"PER\":\n",
    "        m.label = \"PERSON\"\n",
    "    return m.label\n",
    "\n",
    "lex, type_lex, xs_train, ys_train = generate_vecs([x for x in train_mentions if len(x.tokens) >= 5],\n",
    "                                      typ_func,\n",
    "                                      features,\n",
    "                                      dense_feature)\n",
    "\n",
    "_,_, xs_test, ys_test = generate_vecs(test_mentions,\n",
    "                            typ_func,\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)\n",
    "\n",
    "# _,_, xs_dev, ys_dev = generate_vecs(dev_mentions,\n",
    "#                             typ_func,\n",
    "#                             features,\n",
    "#                             dense_feature,\n",
    "#                             lex,\n",
    "#                             type_lex)\n",
    "\n",
    "_,_, xs_figer, _ = generate_vecs(figer_datas,\n",
    "                            lambda m : \"LOC\",\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn import linear_model, datasets, svm, ensemble, naive_bayes\n",
    "\n",
    "# logreg = ensemble.BaggingClassifier(base_estimator=linear_model.Perceptron())\n",
    "logreg = linear_model.Perceptron(penalty=\"l2\", alpha=1e-6)\n",
    "logreg.fit(xs_train, ys_train)\n",
    "y_pred = logreg.predict(xs_test)\n",
    "y_pred_figer = logreg.predict(xs_figer)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def fast_gen_vec(m):\n",
    "    _,_, e, _ = generate_vecs([m],\n",
    "                            lambda m : \"LOC\",\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)\n",
    "    return e\n",
    "\n",
    "def generate_example(tokens):\n",
    "    m = Mention(0, len(tokens), \"\", tokens)\n",
    "    _,_, e, _ = generate_vecs([m],\n",
    "                            lambda m : \"LOC\",\n",
    "                            features,\n",
    "                            dense_feature,\n",
    "                            lex,\n",
    "                            type_lex)\n",
    "    return e\n",
    "\n",
    "print(\"Binary f1  %.3f\" % (f1_score(ys_test, y_pred)))\n",
    "\n",
    "f1 = f1_score(ys_test, y_pred, average=None).tolist()\n",
    "print(f1)\n",
    "rd = {type_lex.m[x]:x for x in type_lex.m}\n",
    "\n",
    "class_names = [None] * len(rd)\n",
    "for i in range(len(rd)):\n",
    "    class_names[i] = rd[i]\n",
    "\n",
    "for k,v in sorted([(rd[k], f1[k]) for k in rd], key=lambda a:a[1], reverse=True):\n",
    "    print(\"%15s : %.3f\" % (k,v)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary f1  0.945\n",
    "[0.9561104805145667, 0.962999185004075, 0.9865470852017937, 0.9855574812247255, 0.928506513177825, 0.676470588235294, 0.9652351738241309, 0.9710485133020343, 0.9, 0.6827309236947792, 0.7031700288184438, 0.9870129870129871, 0.5, 0.9977081741787623, 0.9535353535353536, 0.5730659025787965, 0.7912087912087913, 0.6949152542372882]\n",
    "        PERCENT : 0.998\n",
    "          MONEY : 0.987\n",
    "        ORDINAL : 0.987\n",
    "           DATE : 0.986\n",
    "       CARDINAL : 0.971\n",
    "           NORP : 0.965\n",
    "            GPE : 0.963\n",
    "         PERSON : 0.956\n",
    "           TIME : 0.954\n",
    "            ORG : 0.929\n",
    "       QUANTITY : 0.900\n",
    "            LOC : 0.791\n",
    "    WORK_OF_ART : 0.703\n",
    "            LAW : 0.695\n",
    "          EVENT : 0.683\n",
    "        PRODUCT : 0.676\n",
    "            FAC : 0.573\n",
    "       LANGUAGE : 0.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i,m in enumerate(test_mentions):\n",
    "    if y_pred[i] == ys_test[i]:\n",
    "        continue\n",
    "    if m.label == \"FAC\":\n",
    "#         print(rd[y_pred[i]],m)\n",
    "        counter += 1\n",
    "    if counter == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches =  defaultdict(int)\n",
    "alllabel_matches =  defaultdict(int)\n",
    "\n",
    "match_examples =  defaultdict(list)\n",
    "all_labels = 0\n",
    "for i in range(len(figer_datas)):\n",
    "    preded = rd[y_pred_figer[i]]\n",
    "    fine_types = sorted(figer_datas[i].label.split(\",\"))\n",
    "    alllabel_matches[(preded,\",\".join(fine_types))] += 1\n",
    "    for l in fine_types:\n",
    "        all_labels += 1\n",
    "        matches[(preded, l)] += 1\n",
    "        match_examples[(preded, l)].append(figer_datas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figer_len = float(len(figer_datas))\n",
    "for k in sorted(matches.keys(), key=lambda x : matches[x], reverse=True):\n",
    "    print(k, \"\\t\\t\",matches[k],\"\\t\\t\",matches[k]/figer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_example(t):\n",
    "    return match_examples[t]\n",
    "kk=get_example((('ORG', '/person') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# figer_len = float(len(figer_datas))\n",
    "# for k in sorted(alllabel_matches.keys(), key=lambda x : alllabel_matches[x], reverse=True):\n",
    "#     print(k, \"\\t\\t\",alllabel_matches[k],\"\\t\\t\",alllabel_matches[k]/figer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# figer_datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# ds = []\n",
    "# import json\n",
    "# for i,m in enumerate(figer_datas):\n",
    "#     preded = rd[y_pred_figer[i]]\n",
    "#     s = json.loads(figer_datas[i].__repr__())\n",
    "#     s[\"predicted\"] = preded\n",
    "#     s = json.dumps(s)\n",
    "#     ds.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rd[logreg.predict(generate_example([\"Hirsh\"]))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logreg.coef_[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.asarray(xv_.todense()).flatten() * logreg.coef_[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_r_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14990763,  0.04445518,  0.72226002, ...,  0.6039336 ,\n",
       "         0.88440977,  0.87510776]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((1, xdim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haowu4/anaconda3/envs/dataless_finer/lib/python2.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Perceptron from version pre-0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/haowu4/anaconda3/envs/dataless_finer/lib/python2.7/site-packages/ipykernel/__main__.py:1: DeprecationWarning: The file '/home/haowu4/data/ontonotes_model/logreg.pkl' has been generated with a joblib version less than 0.10. Please regenerate this pickle file.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "m = joblib.load(\"/home/haowu4/data/ontonotes_model/logreg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-42ca1c00ee0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mydim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_r_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: score() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ydim, xdim = m.coef_.shape\n",
    "pred_r_arr = m.score(np.random.random((1, xdim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_revert_map_from_lex(lex):\n",
    "    r = [None] * len(lex.m)\n",
    "    for k in lex.m:\n",
    "        r[lex.m[k]] = k\n",
    "    return r\n",
    "\n",
    "rlex = generate_revert_map_from_lex(lex)\n",
    "\n",
    "def check_detail(example_, kkk_):\n",
    "    xv_ = fast_gen_vec(example_)\n",
    "    print(\"\\n\\n\")\n",
    "    xv_ = np.asarray(xv_.todense()).flatten()\n",
    "    prod_ = xv_ * logreg.coef_[kkk_]\n",
    "    print(\"Score : %.5f\\n\" % (np.sum(prod_) + logreg.intercept_[kkk_]))\n",
    "    inds = np.argsort(-np.abs(prod_))[:200]\n",
    "    \n",
    "    weight_of_w2v = logreg.coef_[kkk_][len(lex.m):].reshape((11,300))\n",
    "    fweight_of_w2v = xv_[len(lex.m):].reshape((11,300))\n",
    "    \n",
    "    w2v_scores = np.sum(weight_of_w2v * fweight_of_w2v, axis = 1).tolist()\n",
    "    \n",
    "    words = []\n",
    "    for i in range(mention.start-pos, mention.start):\n",
    "        if i < 0:\n",
    "            words.append(\"-*-\")\n",
    "        else:\n",
    "            words.append(example_.tokens[i])    \n",
    "    for i in range(mention.end + 1, mention.end + pos + 1):\n",
    "        if i >= len(mention.tokens):\n",
    "            words.append(np.zeros(default_w2v.shape))\n",
    "        else:\n",
    "            words.append(getOrDefault(ws,mention.tokens[i],default_w2v))\n",
    "    \n",
    "    for ind in inds:\n",
    "        if ind >= len(rlex):\n",
    "            fname = \"W2V\"\n",
    "            continue\n",
    "        else:\n",
    "            fname = rlex[ind]\n",
    "        w_ = prod_[ind]\n",
    "        if w_ > 0:\n",
    "            print(\"%-20s : %.4f\" % (fname, w_) )\n",
    "    print( \"----\" * 10)\n",
    "    for ind in inds:\n",
    "        if ind >= len(rlex):\n",
    "            fname = \"W2V\"\n",
    "            continue\n",
    "        else:\n",
    "            fname = rlex[ind]\n",
    "        w_ = prod_[ind]\n",
    "        if w_ < 0:\n",
    "            print(\"%-20s : %.4f\" % (fname, w_) )\n",
    "    print(example_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 67843)\n",
      "dense_vecs[0].shape (300,)\n",
      "dense_vecs.shape (1, 300)\n",
      "shapes :  (1, 67843) (1, 300)\n",
      "\n",
      "\n",
      "\n",
      "Score : -9.45392\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-65ef6bb6d56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_detail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ORG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/person'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-15925d4ff860>\u001b[0m in \u001b[0;36mcheck_detail\u001b[0;34m(example_, kkk_)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mweight_of_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkkk_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfweight_of_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxv_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "check_detail(get_example(('ORG', '/person'))[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\" \".join(get_example(('ORG', '/person'))[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex.counter_per_type[\"dep_feature=-> 55\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pc=0\n",
    "for _ment in train_mentions:\n",
    "#     if _ment.label == \"PERSON\":\n",
    "    for k,v in mention_details(_ment.doc,_ment.start, _ment.end ):\n",
    "        if k == \"dep_feature=-> 55\":\n",
    "            print(_ment)\n",
    "            pc += 1\n",
    "    if pc == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = [x for x in train_mentions if len(x.tokens) == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_m = get_example(('ORG', '/person'))[1]\n",
    "for k in mention_details(_m.doc, _m.start, _m.end ):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_examples= get_example((('ORG', '/person') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex.counter[\"dep_feature=<-spend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lex.counter_per_type[\"dep_feature=<-spend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedfeatures = sorted(lex.counter.keys(), key=lambda x:lex.counter[x], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in sortedfeatures[:100]:\n",
    "    print(\"%-28s : %d\" % (x, lex.counter[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type_lex.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/home/haowu4/data/ontonotes_model/w2vdict\",'r') as inp:\n",
    "    new_w2dict = pickle.load(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w2dict[\"Hello\"] - w2vdict[\"Hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open(\"/home/haowu4/data/ontonotes_model/type_lex.pkl\",'r') as inp:\n",
    "    ty_lex_pk = pickle.load(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'CARDINAL': 8,\n",
       " u'DATE': 4,\n",
       " u'EVENT': 6,\n",
       " u'FAC': 14,\n",
       " u'GPE': 1,\n",
       " u'LANGUAGE': 16,\n",
       " u'LAW': 15,\n",
       " u'LOC': 10,\n",
       " u'MONEY': 13,\n",
       " u'NORP': 3,\n",
       " u'ORDINAL': 5,\n",
       " u'ORG': 7,\n",
       " u'PERCENT': 11,\n",
       " u'PERSON': 0,\n",
       " u'PRODUCT': 17,\n",
       " u'QUANTITY': 12,\n",
       " u'TIME': 2,\n",
       " u'WORK_OF_ART': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty_lex_pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dataless_finer]",
   "language": "python",
   "name": "conda-env-dataless_finer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
